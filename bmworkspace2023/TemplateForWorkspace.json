{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "bmworkspace2023"
		},
		"bmworkspace2023-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bmworkspace2023-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:bmworkspace2023.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bmdatalakestorage2023.dfs.core.windows.net/"
		},
		"bmworkspace2023-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bmdatalakestorage2023.dfs.core.windows.net"
		},
		"ls_bmehubkv2023_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://bmehubkeyvault2023.vault.azure.net/"
		},
		"Trigger 1_properties_typeProperties_scope": {
			"type": "string",
			"defaultValue": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Storage/storageAccounts/bmdatalakestorage2023"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/PL_EOY')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "TransformEOYData",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "eoy_trans_run",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "bmsparkpool2023",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 1
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					},
					{
						"name": "LoadToNeo4J",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "TransformEOYData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "eoy_run",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "bmsparkpool2023",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 1
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2023-02-22T10:33:36Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/eoy_trans_run')]",
				"[concat(variables('workspaceId'), '/bigDataPools/bmsparkpool2023')]",
				"[concat(variables('workspaceId'), '/notebooks/eoy_run')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ent_run",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ent_run",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "ent_load_run",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "ent_run",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ent_load_run",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2023-02-20T10:54:21Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/ent_run')]",
				"[concat(variables('workspaceId'), '/notebooks/ent_load_run')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bmworkspace2023-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bmworkspace2023-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bmworkspace2023-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bmworkspace2023-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_bmehubkv2023')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('ls_bmehubkv2023_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "BlobEventsTrigger",
				"typeProperties": {
					"blobPathBeginsWith": "/test/blobs/",
					"ignoreEmptyBlobs": true,
					"scope": "[parameters('Trigger 1_properties_typeProperties_scope')]",
					"events": [
						"Microsoft.Storage.BlobCreated"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EOY_Config_Update')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "how to merge common config with data source config.",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2fcfe57c-16d7-4a1b-8f85-7cfa5f7f2223"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"storage.account_name\"])\r\n",
							"#print(e_config)\r\n",
							"cm_config = config\r\n",
							"print(cm_config)"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"data.chunksize\"])\r\n",
							"config.update(cm_config)\r\n",
							"print(config)\r\n",
							"print(config[\"storage.account_name\"])"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import pandas as pd\r\n",
							"import sql\r\n",
							"data_chunks  = pd.read_csv('abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/lookup_data/EOY_emp_key_lookup.csv',chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"type(data_chunks)\r\n",
							"for data_chunk in data_chunks:\r\n",
							"    count(data_chunk)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EOY_emp_key_lookup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "41a5baf6-3775-4336-ae88-aca38088c2d6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load('abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/lookup_data/EOY_emp_key_lookup.csv',format='csv',header=True)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ETL_Logger')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9548a03f-8fe9-412e-90f0-4492d35f228c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import json\r\n",
							"from datetime import datetime\r\n",
							"from azure.data.tables import TableClient\r\n",
							"\r\n",
							"class ETLlogger:\r\n",
							"    def __init__(self,_config):\r\n",
							"        self._config = _config\r\n",
							"        \"\"\"Connect to Azure table\"\"\"\r\n",
							"        self.connection_str = \"DefaultEndpointsProtocol=https;AccountName=\" + _config.get('storage.account_name') + \";AccountKey=\" + \\\r\n",
							"                         _config.get('storage.account_key') + \";EndpointSuffix=core.windows.net\"\r\n",
							"        self.table_service = TableClient.from_connection_string(self.connection_str,\r\n",
							"                                                           table_name=_config.get('logging.table_name'))\r\n",
							"\r\n",
							"    def update_logs(self, task, result):\r\n",
							"        tday = datetime.now()\r\n",
							"        partition_key = tday.strftime(\"%Y%m%d\")\r\n",
							"        row_key = self._config.get('source_flag') + \"-\" + tday.strftime(\"%Y%m%d-%H%M%S\")\r\n",
							"        properties = {\r\n",
							"            'source': self._config.get('source_flag'),\r\n",
							"            'task': task,\r\n",
							"            'result': result\r\n",
							"        }\r\n",
							"\r\n",
							"        entity_values = {'PartitionKey': partition_key,\r\n",
							"                         'RowKey': row_key,\r\n",
							"                         'Result': json.dumps(properties)}\r\n",
							"\r\n",
							"        self.table_service.create_entity(entity=entity_values)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-data-tables"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Neo4jSynapseConn')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "bf3c20ee-003b-411d-bae2-cbf20a94f52b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"neo4j_bolt_url: \"bolt://54.196.135.240:7687\"\r\n",
							"neo4j_user_name: \"neo4j\"\r\n",
							"neo4j_password: \"diesel-velocities-originals\"\r\n",
							"neo4j_database: \"Blank Sandbox\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run sampleCypher"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(cypher2)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher_csv = (\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/test/data/dummy/person_data.csv?sp=r&st=2023-02-17T11:05:31Z&se=2023-02-17T19:05:31Z&spr=https&sv=2021-06-08&sr=b&sig=kjskARl%2BkBN4W4KvE9mQppOS88ZnTEG1hDFBi58%2B358%3D\"  AS line \r\n",
							"WITH line\r\n",
							"MERGE (per:person{pname:line.name})\r\n",
							"ON CREATE SET per.pdept = line.dept\"\"\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher_csv2 =\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/test/data/landing/ent/ent/part-00002.csv?se=2023-02-17T11%3A18%3A09Z&sp=r&sv=2021-08-06&ss=b&srt=o&sig=oYHl8YLhkwbxP7ndmz4rU%2BDQvOweRItMZntgbiF0lXU%3D\"  AS line  \r\n",
							"\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"\r\n",
							"MATCH (ey:EY:Organization{key:'ey'})\r\n",
							"\r\n",
							"MERGE (per:Person{key:line.per_key})\r\n",
							"ON CREATE SET per.name = line.entrepreneurs_name\r\n",
							"ON CREATE SET per.email = line.entrepreneurs_email\r\n",
							"ON CREATE SET per.phone = line.entrepreneurs_mobile_phone\r\n",
							"ON CREATE SET per.createdAt = timeStamp\r\n",
							"SET per.updatedAt = timeStamp\r\n",
							"\r\n",
							"MERGE (psv:PersonSourceVersion:Version{id:'ENT'+ line.per_id, source:'ent'})\r\n",
							"ON CREATE SET psv.createdAt = timeStamp\r\n",
							"SET psv.updatedAt = timeStamp\r\n",
							"SET psv.name = line.entrepreneurs_name\r\n",
							"SET psv.email = line.entrepreneurs_email\r\n",
							"SET psv.phone = line.entrepreneurs_mobile_phone\r\n",
							"\r\n",
							"MERGE (per)-[:HAS_VERSION]->(psv)\r\n",
							"\r\n",
							"Merge (org:Organization{key:line.org_key})\r\n",
							"ON CREATE SET org.name = line.company_name\r\n",
							"ON CREATE SET org.address = line.company_address\r\n",
							"ON CREATE SET org.postalCode = line.company_postal_code\r\n",
							"ON CREATE SET org.phone = line.company_telephone\r\n",
							"ON CREATE SET org.foundedOn = line.company_founded_on\r\n",
							"ON CREATE SET org.homepageURL = line.company_website\r\n",
							"ON CREATE SET org.createdAt = timeStamp\r\n",
							"SET org.updatedAt = timeStamp\r\n",
							"SET org.tradingName = line.company_trading_name\r\n",
							"SET org.isOwned = CASE line.company_owned_flag WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END \r\n",
							"SET org.ownedBy = line.company_owned_name\r\n",
							"SET org.geoReach = line.company_geo_reach\r\n",
							"SET org.duns = line.company_duns_no\r\n",
							"SET org.holding = CASE WHEN trim(COALESCE(line.company_public_private_flag, '')) <> '' THEN  trim(line.company_public_private_flag) ELSE NULL END\r\n",
							"SET org.goPublicYear = line.company_go_public_year\r\n",
							"SET org.tradingSymbol = line.company_trading_symbol\r\n",
							"SET org.connectionToEYType = 'direct'\r\n",
							"\r\n",
							"MERGE (osv:OrgSourceVersion:Version{id:'ENT'+line.org_id, source:'ent'})\r\n",
							"ON CREATE SET osv.createdAt = timeStamp\r\n",
							"SET osv.updatedAt = timeStamp\r\n",
							"SET osv.name = line.company_name\r\n",
							"SET osv.tradingName = line.company_trading_name\r\n",
							"SET osv.address = line.company_address\r\n",
							"SET osv.country = line.company_country\r\n",
							"SET osv.postalCode = line.company_postal_code\r\n",
							"SET osv.region = line.company_region_state\r\n",
							"SET osv.city = line.company_city\r\n",
							"SET osv.phone = line.company_telephone\r\n",
							"SET osv.socialMedia = line.social_media\r\n",
							"SET osv.isOwned = CASE line.company_owned_flag WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END \r\n",
							"SET osv.ownedBy = line.company_owned_name\r\n",
							"SET osv.foundedOn = line.company_founded_on\r\n",
							"SET osv.geoReach = line.company_geo_reach\r\n",
							"SET osv.duns = line.company_duns_no\r\n",
							"SET osv.homepageURL = line.company_website\r\n",
							"SET osv.holding = CASE WHEN trim(COALESCE(line.company_public_private_flag, '')) <> '' THEN  trim(line.company_public_private_flag) ELSE NULL END\r\n",
							"SET osv.goPublicYear = line.company_go_public_year\r\n",
							"SET osv.tradingSymbol = line.company_trading_symbol\r\n",
							"\r\n",
							"MERGE (org)-[:HAS_VERSION]->(osv)\r\n",
							"\r\n",
							"MERGE (per)-[wi:WORKS_IN]->(org)\r\n",
							"ON CREATE SET wi.source = 'ent'\r\n",
							"SET wi.jobTitle = line.entrepreneurs_job_title\r\n",
							"SET wi.phone = line.entrepreneurs_work_phone\r\n",
							"SET wi.isFounder = CASE line.entrepreneurs_company_founder WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END\r\n",
							"SET wi.percentOwned = line.entrepreneurs_company_founder\r\n",
							"\r\n",
							"MERGE (e:Event{key:line.event_key})\r\n",
							"ON CREATE SET e.createdAt = timeStamp\r\n",
							"SET e.updatedAt = timeStamp\r\n",
							"SET e.name = line.event_name\r\n",
							"SET e.type = CASE WHEN line.source = 'EOY UX' THEN 'EOY' ELSE line.source END\r\n",
							"SET e.area = line.event_area\r\n",
							"SET e.region = line.event_region\r\n",
							"SET e.year = line.event_year\r\n",
							"SET e.parentSource = 'ent'\r\n",
							"SET e.source = CASE WHEN line.source = 'EOY UX' THEN 'eoy' ELSE TOLOWER(line.source) END\r\n",
							"\r\n",
							"MERGE (per)-[pip:IS_PARTICIPANT]->(e)\r\n",
							"ON CREATE SET pip.createdAt = timeStamp\r\n",
							"SET pip.updatedAt = timeStamp\r\n",
							"SET pip.nominationId = line.event_nomination_id\r\n",
							"SET pip.nominationDate = line.event_nomination_date\r\n",
							"SET pip.source = 'ent'\r\n",
							"\r\n",
							"MERGE (org)-[oip:IS_PARTICIPANT]->(e)\r\n",
							"ON CREATE SET oip.createdAt = timeStamp\r\n",
							"SET oip.updatedAt = timeStamp\r\n",
							"SET oip.nominationId = line.event_nomination_id\r\n",
							"SET oip.nominationDate = line.event_nomination_date\r\n",
							"SET oip.source = 'ent'\r\n",
							"\r\n",
							"// EY Alumni\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_alumini_tier IS NOT NULL AND line.ey_alumini_tier <> '' THEN [1] ELSE [] END | \r\n",
							"    MERGE (per)-[awi:WORKS_IN]->(ey)\r\n",
							"    SET awi.source = 'ent'\r\n",
							"    SET awi.isActive = false\r\n",
							"    SET awi.isPrimary = false \r\n",
							"    SET awi.tier = line.ey_alumini_tier\r\n",
							")\r\n",
							"\r\n",
							"// Other EY data\r\n",
							"FOREACH(ignore IN CASE WHEN line.is_ey IN ['True', 'TRUE', 'true', true] THEN [1] ELSE [] END | \r\n",
							"    MERGE (e)-[ict:IS_CONNECTED_TO]->(ey)\r\n",
							"    SET ict.relationshipManager = line.ey_relationship_mgr\r\n",
							"    SET ict.host = line.ey_host\r\n",
							"    SET ict.lastTargetDate = line.ey_last_target_date\r\n",
							"    SET ict.targetNotes = line.ey_target_notes\r\n",
							"    SET ict.gnFlag = line.ey_growth_navigator_flag\r\n",
							"    SET ict.gnDate = line.ey_growth_navigator_date\r\n",
							"    SET ict.familyBiz = line.ey_family_biz_flag\r\n",
							")\r\n",
							"\r\n",
							"// EY Client\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_client_flag = 'Yes' THEN [1] ELSE [] END |\r\n",
							"    MERGE (org)-[:HAS_CUSTOMER_INFO{source:'ent'}]->(ci:CustomerInfo)\r\n",
							"    ON CREATE SET ci.id = line.ey_client_id\r\n",
							"    ON CREATE SET ci.createdAt = timeStamp\r\n",
							"    SET ci.updatedAt = timeStamp \r\n",
							"    SET ci.channel = line.ey_channel_nm\r\n",
							"    SET ci.marketSegment = line.ey_market_segment\r\n",
							"\r\n",
							"    SET osv.channel = line.ey_channel_nm\r\n",
							"    SET osv.clientId = line.ey_client_id\r\n",
							"\r\n",
							"    SET org.channel = line.ey_channel_nm\r\n",
							"    SET org.clientId = line.ey_client_id\r\n",
							")\r\n",
							"\r\n",
							"// EY IndustrySector (ey_industry transformed - & to and)\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_industry IS NOT NULL AND line.ey_industry <> '' THEN [1] ELSE [] END | \r\n",
							"    MERGE(sec:Sector{id:TOLOWER(line.ey_industry)})\r\n",
							"    ON CREATE SET sec.name = line.ey_industry\r\n",
							"    ON CREATE SET sec.createdAt = timeStamp\r\n",
							"    SET sec.updatedAt = timeStamp\r\n",
							"    \r\n",
							"    MERGE (org)-[ii:IS_IN]->(sec)\r\n",
							"    ON CREATE SET ii.createdAt = timeStamp\r\n",
							"    SET ii.updatedAt = timeStamp\r\n",
							"    SET ii.source = 'ent')\"\"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"query_dict = {\"organizations\":cypher_csv}"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load using single cypher query at a time stored in notebook sampleCypher and sampleCypher5\r\n",
							"from neo4j import GraphDatabase\r\n",
							"\r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.196.135.240:7687\", auth=('neo4j',\"diesel-velocities-originals\"))  \r\n",
							"entities = ['organizations']\r\n",
							"# cypherQueryList = []\r\n",
							"#sas_url_val = \"https://bmdatalakestorage2023.blob.core.windows.net/test/data/dummy/person_data.csv?sp=r&st=2023-02-16T10:10:25Z&se=2023-02-17T18:10:25Z&spr=https&sv=2021-06-08&sr=b&sig=n4gT4GV%2FXJZaktjsvArNQbDYVCpVL%2FUJkDX0CJ3gPiU%3D\"\r\n",
							"\r\n",
							"for entity in entities:\r\n",
							"    # cypherQueryList = [cypher_csv]\r\n",
							"    cypherQuery = query_dict[entity]\r\n",
							"    print(cypherQuery)\r\n",
							"    #cypherQuery = cypherQuery.replace('<<SAS_URL>>',sas_url_val)\r\n",
							"    #print(cypherQuery)\r\n",
							"    with connector.session(database = 'neo4j') as session:\r\n",
							"        session.write_transaction(execute_txn,cypherQuery)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load for erms data with referencing sampleCypher Notebook\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.196.135.240:7687\", auth=('neo4j',\"diesel-velocities-originals\"))  \r\n",
							"entities = ['organizations']  \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"\r\n",
							"for entity in entities:\r\n",
							"    # cypherQueryList = [cypher_csv]\r\n",
							"    cypherQueryList = query_dict[entity]\r\n",
							"    with connector.session(database = 'neo4j') as session:\r\n",
							"        session.write_transaction(execute_txn,cypherQueryList)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load using cypher query list stored in notebook sampleCypher and sampleCypher5\r\n",
							"from neo4j import GraphDatabase\r\n",
							"\r\n",
							"def execute_txn(tx,*queryList):\r\n",
							"    for query in queryList:\r\n",
							"        res = tx.run(query)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.196.135.240:7687\", auth=('neo4j',\"diesel-velocities-originals\"))  \r\n",
							"entities = ['organizations','degrees']\r\n",
							"cypherQueryList = []\r\n",
							"\r\n",
							"for entity in entities:\r\n",
							"    # cypherQueryList = [cypher_csv]\r\n",
							"    cypherQueryList = query_dict[entity]\r\n",
							"    print(cypherQueryList)\r\n",
							"    with connector.session(database = 'neo4j') as session:\r\n",
							"        session.write_transaction(execute_txn,*cypherQueryList)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sas_url = \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\"\r\n",
							"   \r\n",
							"cypher = (\"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\" AS line\r\n",
							"    WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"    MERGE (norg:Organization{key:line.org_key})\r\n",
							"    ON CREATE SET norg.role = line.role\r\n",
							"    ON CREATE SET norg.createdAt = timeStamp\r\n",
							"    SET norg.updatedAt = timeStamp\r\n",
							"    SET norg.name = line.name\r\n",
							"    SET norg.role = line.role \r\n",
							"    SET norg.type = line.type\"\"\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ermsCypher"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load for erms data with referencing ermsCypher Notebook\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session(database = 'neo4j') as session:\r\n",
							"    session.write_transaction(execute_txn,cypher)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run sampleCypher"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load for erms data with referencing sampleCypher Notebook\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session(database = 'neo4j') as session:\r\n",
							"    session.write_transaction(execute_txn,cypher2)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run sampleCypher5"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load using cypher query stored in notebook sampleCypher and sampleCypher5\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,*queryList):\r\n",
							"    for query in queryList:\r\n",
							"        res = tx.run(query)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"with connector.session(database = 'neo4j') as session:\r\n",
							"    cypherQueryList = [cypher5,cypher2,cypher]\r\n",
							"    session.write_transaction(execute_txn,*cypherQueryList)"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct load for erms data\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session(database = 'neo4j') as session:\r\n",
							"    cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023/data/landing/erms/organizations_inc/part-00000.csv?sp=r&st=2023-02-01T11:50:20Z&se=2023-02-02T19:50:20Z&spr=https&sv=2021-06-08&sr=b&sig=Q764ixiccprFrjAf5T5UcptEt4aXXPL%2BE5vsNut9jkU%3D\" AS line\r\n",
							"    WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"    MERGE (norg:Organization{key:line.org_key})\r\n",
							"    ON CREATE SET norg.role = line.role\"\"\"\r\n",
							"    session.write_transaction(execute_txn,cypher)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"\r\n",
							"# URI examples: \"neo4j://localhost\", \"neo4j+s://xxx.databases.neo4j.io\"\r\n",
							"URI = \"bolt://54.165.7.198:7687\"\r\n",
							"AUTH = (\"neo4j\", \"restriction-component-descriptions\")\r\n",
							"\r\n",
							"with GraphDatabase.driver(URI, auth=AUTH) as driver:\r\n",
							"\r\n",
							"\r\n",
							"    with driver.session(database=\"Blank Sandbox\") as session:\r\n",
							"        \r\n",
							"\r\n",
							"        driver.verify_connectivity()"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" from neo4j import GraphDatabase\r\n",
							" def execute_txn(tx, query):\r\n",
							"     connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))\r\n",
							"     cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"     WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"     MERGE (norg:Organization{key:line.org_key})\r\n",
							"     ON CREATE SET norg.role = line.role\r\n",
							"     ON CREATE SET norg.createdAt = timeStamp\r\n",
							"     SET norg.updatedAt = timeStamp\r\n",
							"     SET norg.name = line.name\r\n",
							"     SET norg.role = line.role \r\n",
							"     SET norg.type = line.type\"\"\" \r\n",
							"     with connector.session(database=\"Blank Sandbox\") as session:\r\n",
							"         session.write_transaction(execute_txn,cypher)\r\n",
							"     res = tx.run(query)\r\n",
							"     values = []\r\n",
							"     for record in res:\r\n",
							"         values.append(dict(record))"
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#correct code for sample data\r\n",
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session(database='neo4j') as session:\r\n",
							"    cypher = \"CREATE (a:Person {name: 'John Doe'})\"\r\n",
							"    session.write_transaction(execute_txn,cypher)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session(database=\"Blank Sandbox\") as session:\r\n",
							"    cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"     WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"     MERGE (norg:Organization{key:line.org_key})\r\n",
							"     ON CREATE SET norg.role = line.role\r\n",
							"     ON CREATE SET norg.createdAt = timeStamp\r\n",
							"     SET norg.updatedAt = timeStamp\r\n",
							"     SET norg.name = line.name\r\n",
							"     SET norg.role = line.role \r\n",
							"     SET norg.type = line.type\"\"\"  \r\n",
							"    session.write_transaction(execute_txn,cypher)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))\r\n",
							"#cypher =  \r\n",
							"def create_node(tx):\r\n",
							"    tx.run(\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\")\r\n",
							"with connector.session(database=\"Blank Sandbox\") as session:\r\n",
							"    \r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#def build_query(sas_url):\r\n",
							"sas_url = \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\"\r\n",
							"cypher = (\"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\")\r\n",
							"    #query = cypher.replace('<<SAS_URL>>', sas_url)\r\n",
							"    #return query"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\""
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"import os\r\n",
							"import time\r\n",
							"\r\n",
							"class GraphAdapter:\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._connector = GraphDatabase.driver(self._config['neo4j.bolt_url'], \r\n",
							"        auth=(self._config['neo4j.user_name'], self._config['neo4j.password']),\r\n",
							"        encrypted=False)\r\n",
							"\r\n",
							"    def execute_query(self, query):\r\n",
							"        connector = None\r\n",
							"        try:\r\n",
							"            self._logger.info(f\"Query execution initiated.\")\r\n",
							"            start = time.time()\r\n",
							"            with self._connector.session(database=self._config['neo4j_database']) as session:\r\n",
							"                res = session.write_transaction(self._execute_txn, query)\r\n",
							"            end = time.time()\r\n",
							"            self._logger.info(f\"Query execution completed. Execution Time - {end - start} seconds.\")\r\n",
							"            return res\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(f\"Query Execution Failed - {e}\")\r\n",
							"\r\n",
							"        \r\n",
							"    def _execute_txn(self, tx, query):\r\n",
							"        res = tx.run(query)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"        return values\r\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run neo_connector"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#from neo4j import Graph\r\n",
							"from neo4j import GraphDatabase\r\n",
							"#sas_url = \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\"\r\n",
							"#if sas_url:\r\n",
							"    #query = build_query(sas_url)\r\n",
							"if query:\r\n",
							"    \r\n",
							"\r\n",
							"    graph.execute_query(query)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"neo4j_bolt_url: \"bolt://54.165.7.198:7687\"\r\n",
							"neo4j_user_name: \"neo4j\"\r\n",
							"neo4j_password: \"restriction-component-descriptions\"\r\n",
							"neo4j_database: \"Blank Sandbox\""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\""
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"import os\r\n",
							"import time\r\n",
							"\r\n",
							"class GraphAdapter:\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._connector = GraphDatabase.driver(self._config['neo4j_bolt_url'], auth=(self._config['neo4j_user_name'], self._config['neo4j_password']),encrypted=False)\r\n",
							"\r\n",
							"    def execute_query(self, query):\r\n",
							"        connector = None\r\n",
							"        try:\r\n",
							"            self._logger.info(f\"Query execution initiated.\")\r\n",
							"            start = time.time()\r\n",
							"            with self._connector.session(database=self._config['neo4j_database']) as session:\r\n",
							"                res = session.write_transaction(execute_txn, cypher)\r\n",
							"            end = time.time()\r\n",
							"            self._logger.info(f\"Query execution completed. Execution Time - {end - start} seconds.\")\r\n",
							"            return res\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(f\"Query Execution Failed - {e}\")\r\n",
							"\r\n",
							"        \r\n",
							"    def execute_txn(tx, query):\r\n",
							"        res = tx.run(query)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"        return values\r\n",
							""
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"driver = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=(\"neo4j\",\"restriction-component-descriptions\"))\r\n",
							"def create_node(tx):\r\n",
							"    tx.run(\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\")\r\n",
							"with driver.session() as session:\r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sas_url = \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\"\r\n",
							"cypher = (\"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role \r\n",
							"SET norg.type = line.type\"\"\")\r\n",
							"query = cypher.replace('<<SAS_URL>>', sas_url)\r\n",
							"driver = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=(\"neo4j\",\"restriction-component-descriptions\"))\r\n",
							"def create_node(tx):\r\n",
							"    tx.run(query)\r\n",
							"with driver.session() as session:\r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"driver = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=(\"neo4j\",\"restriction-component-descriptions\"))\r\n",
							"def create_node(tx):\r\n",
							"    tx.run(\"CREATE (a:Person {name: 'John Doe'})\")\r\n",
							"with driver.session() as session:\r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"connector = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=('neo4j',\"restriction-component-descriptions\"))   \r\n",
							"def execute_txn(tx,query):\r\n",
							"    res = tx.run(query)\r\n",
							"    values = []\r\n",
							"    for record in res:\r\n",
							"        values.append(dict(record))\r\n",
							"with connector.session() as session:\r\n",
							"    session.write_transaction(execute_txn,cypher)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"driver = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=(\"neo4j\",\"restriction-component-descriptions\"))\r\n",
							"def create_node(tx):\r\n",
							"    tx.run(\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"    WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"    MERGE (norg:Organization{key:line.org_key})\r\n",
							"    ON CREATE SET norg.role = line.role\r\n",
							"    ON CREATE SET norg.createdAt = timeStamp\r\n",
							"    SET norg.updatedAt = timeStamp\r\n",
							"    SET norg.name = line.name\r\n",
							"    SET norg.role = line.role \r\n",
							"    SET norg.type = line.type\"\"\")\r\n",
							"with driver.session() as session:\r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023?sp=r&st=2023-01-31T08:12:55Z&se=2023-02-03T16:12:55Z&spr=https&sv=2021-06-08&sr=c&sig=epfAUlvBGYspMim2vQFxPlm1LkbJyndeFkDS9HVL5kk%3D\" AS line\r\n",
							"    WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"    MERGE (norg:Organization{key:line.org_key})\r\n",
							"    ON CREATE SET norg.role = line.role\r\n",
							"    ON CREATE SET norg.createdAt = timeStamp\r\n",
							"    SET norg.updatedAt = timeStamp\r\n",
							"    SET norg.name = line.name\r\n",
							"    SET norg.role = line.role \r\n",
							"    SET norg.type = line.type\"\"\""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"driver = GraphDatabase.driver(\"bolt://54.165.7.198:7687\", auth=(\"neo4j\",\"restriction-component-descriptions\"))\r\n",
							"def create_node(tx):\r\n",
							"    tx.run(\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\" AS line\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp,\r\n",
							"MERGE (norg:Organization{key:line.org_key})\r\n",
							"ON CREATE SET norg.role = line.role\\\r\n",
							"ON CREATE SET norg.createdAt = timeStamp\r\n",
							"SET norg.updatedAt = timeStamp\r\n",
							"SET norg.name = line.name\r\n",
							"SET norg.role = line.role\r\n",
							"SET norg.type = line.type\")\r\n",
							"with driver.session() as session:\r\n",
							"\r\n",
							"    session.write_transaction(create_node)"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1-rough-eoy-trans')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0d5166d3-f278-46c0-b250-4cce548509e5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import pandas as pd\r\n",
							"source = 'EOY'\r\n",
							"file_name = 'emp_key_lookup.csv'\r\n",
							"file_name2 = source+'_'+(file_name)\r\n",
							"file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/lookup_data/' + file_name2\r\n",
							"\r\n",
							"#data_chunks  = pd.read_csv(file_path,chunksize=1000 , dtype=str)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source = 'EOY'\r\n",
							"file_name = 'emp_key_lookup.csv'\r\n",
							"file_name2 = source+'_'+(file_name)\r\n",
							"print(file_name2)\r\n",
							"file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/lookup_data/' + file_name2\r\n",
							"print(file_path)"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/az_table_connector"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"storage.account_name\"])\r\n",
							"#print(e_config)\r\n",
							"cm_config = config\r\n",
							"print(cm_config)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"data.chunksize\"])\r\n",
							"config.update(cm_config)\r\n",
							"print(config)\r\n",
							"print(config[\"storage.account_name\"])\r\n",
							"print(config[\"storage.lookup_dir\"])"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"print(storageaccountname)\r\n",
							"storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"print(storagecontainername)\r\n",
							"storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"print(storageaccountkey)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source = config['source_flag']\r\n",
							"print(source)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"storage.lookup_dir\"])"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"lookup_directory = config[\"storage.lookup_dir\"]\r\n",
							"print(lookup_directory)\r\n",
							"print(config[\"storage.container_name\"])"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import pandas as pd\r\n",
							"source = config['source_flag']\r\n",
							"file_name = 'emp_key_lookup.csv'\r\n",
							"file_name2 = source+'_'+(file_name)\r\n",
							"#print(file_name2)\r\n",
							"file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/lookup_data/' + lookup_directory + file_name2\r\n",
							"print(file_path)\r\n",
							"lookup_directory = config[\"storage.lookup_dir\"]\r\n",
							"data_chunks  = pd.read_csv('abfss://test@bmdatalakestorage2023.dfs.core.windows.net/'+lookup_directory+/'EOY_emp_key_lookup.csv',chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"type(data_chunks)\r\n",
							"for data_chunk in data_chunks:\r\n",
							"    display(data_chunk)"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source = config['source_flag']\r\n",
							"file_name = 'emp_key_lookup.csv'\r\n",
							"file_name2 = source+'_'+(file_name)\r\n",
							"lookup_directory = config[\"storage.lookup_dir\"]\r\n",
							"file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/'+lookup_directory+'/'+file_name2\r\n",
							"file_path2 = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"print(file_path)\r\n",
							"print(file_path2)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"file_name = 'eoy.csv'\r\n",
							"\r\n",
							"file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"#print(file_path)\r\n",
							"data_chunks  = pd.read_csv(file_path, chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"for data in data_chunks:\r\n",
							"    display(data)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"file_name = 'emp_key_lookup.csv'\r\n",
							"file_path2 = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"print(file_path2)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]\r\n",
							"print(dest_path)"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_transformation_lookup = config[\"transformations\"]\r\n",
							"print(_transformation_lookup)"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_transformation_lookup = config[\"transformations\"]\r\n",
							"#print(_transformation_lookup)\r\n",
							"\r\n",
							"transformations =  _transformation_lookup.get('mappings', [])\r\n",
							"#print(transformations)\r\n",
							"print(_transformation_lookup.get('eoy',{}).get('mappings'))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import re\r\n",
							"import csv\r\n",
							"import traceback\r\n",
							"file_name = 'eoy.csv'\r\n",
							"\r\n",
							"\r\n",
							"file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"\r\n",
							"data_chunks  = pd.read_csv(file_path, chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"for data in data_chunks:\r\n",
							"    display(data)\r\n",
							"        \r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_transformation_lookup = config[\"transformations\"]\r\n",
							"transformations =  _transformation_lookup.get('mappings', [])\r\n",
							"for trfm in transformations2:\r\n",
							"    fun = trfm[\"function\"]\r\n",
							"    src_cols = trfm[\"src_column_names\"]\r\n",
							"    dst_cols = trfm[\"dst_column_names\"]\r\n",
							"    print(trfm[\"src_column_names\"])\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]\r\n",
							"print(dest_path)\r\n",
							"print(config[\"storage.landing_dir_name\"])\r\n",
							"print(config[\"storage.temp_dir\"])"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"storage.chunksize\"])\r\n",
							"print('abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import pandas as pd\r\n",
							"import csv\r\n",
							"entity='eoy'\r\n",
							"source_file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"#chunksize = config[\"storage.chunksize\"]\r\n",
							"#fileuploadname = 'part-'\r\n",
							"data_chunks  = pd.read_csv(source_file_path, chunksize=config[\"storage.chunksize\"], dtype=str)\r\n",
							"print(data_chunks)\r\n",
							"#_file_cnt = 0\r\n",
							"for chunks in data_chunks:\r\n",
							"    #data = pd.DataFrame(chunks)\r\n",
							"    display(chunks)\r\n",
							"    #data = io.StringIO()\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#split upload the file to landing layer as part files\r\n",
							"import pandas as pd\r\n",
							"import csv\r\n",
							"entity='eoy'\r\n",
							"source_file_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"fileuploadname = 'part-'\r\n",
							"data_chunks  = pd.read_csv(source_file_path, chunksize = config[\"storage.chunksize\"], dtype=str)\r\n",
							"_file_cnt = 0\r\n",
							"for chunks in data_chunks:\r\n",
							"    data = pd.DataFrame(chunks)\r\n",
							"    uploadFileName = fileuploadname + str(_file_cnt).rjust(5,'0')\r\n",
							"    landing_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]+'/'+entity+'/'+uploadFileName+'.csv'\r\n",
							"    #data.to_csv(landing_path,index=False,storage_options = {'account_key':'Wl+cozJ6xDQtnstjWpmHpz/2UGxgaptMpDl1/OMPiLUQS+2fuNdeqMmnvzgCTXTwL7c3oNYjrRvl+AStI26Zaw=='})\r\n",
							"    data.to_csv(landing_path,index=False)\r\n",
							"    _file_cnt+=1\r\n",
							"    "
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config[\"storage.landing_dir_name\"])"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import re\r\n",
							"import csv\r\n",
							"import traceback\r\n",
							"from io import BytesIO \r\n",
							"\r\n",
							"class TransformationController:\r\n",
							"\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self.config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._org_key_lookup = pd.DataFrame()\r\n",
							"        self._emp_key_lookup = pd.DataFrame()\r\n",
							"        self.transform()\r\n",
							"\r\n",
							"    def transform(self, file_path=None, entity='eoy'):\r\n",
							"        file_name = 'eoy.csv'\r\n",
							"\r\n",
							"        #file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"        file_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"\r\n",
							"\r\n",
							"        data_chunks  = pd.read_csv(file_path, chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"\r\n",
							"        #file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/staging/eoy/eoy.csv'\r\n",
							"        #_transformation_lookup = config[\"transformations\"].get(entity, {})\r\n",
							"        #transformations =  _transformation_lookup.get('mappings', [])\r\n",
							"        _transformation_lookup = config[\"transformations\"]\r\n",
							"        transformations = _transformation_lookup.get(entity,{}).get('mappings',[])\r\n",
							"        #print(transformations)\r\n",
							"    \r\n",
							"        \r\n",
							"       \r\n",
							"        final_chunk = []\r\n",
							"\r\n",
							"        for data_chunk in data_chunks:\r\n",
							"            \r\n",
							"            data_chunk.dropna(how='all', inplace=True)\r\n",
							"            data_chunk.fillna('', inplace=True)\r\n",
							"            for trfm in transformations:\r\n",
							"                try:\r\n",
							"                    \r\n",
							"                    fun = trfm[\"function\"]\r\n",
							"                    src_cols = trfm[\"src_column_names\"]\r\n",
							"                    dst_cols = trfm[\"dst_column_names\"]\r\n",
							"                    \r\n",
							"                    if \"threshold\" in trfm:\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun), args=(trfm[\"threshold\"],))\r\n",
							"                    elif isinstance(src_cols, list):\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun), axis=1)\r\n",
							"                    else:\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun))\r\n",
							"                except Exception as e:\r\n",
							"                    self._logger.error(str(e))\r\n",
							"                    traceback.print_exc()\r\n",
							"\r\n",
							"\r\n",
							"            final_chunk.append(data_chunk)\r\n",
							"        if final_chunk:\r\n",
							"            _data = pd.concat(final_chunk, ignore_index=True)\r\n",
							"\r\n",
							"            invalid_columns = _transformation_lookup.get('invalid_columns', [])\r\n",
							"            if invalid_columns:\r\n",
							"\r\n",
							"\r\n",
							"                _data.drop(invalid_columns, axis=1, inplace=True)\r\n",
							"\r\n",
							"            #local_file_path = os.path.join(tempfile.gettempdir(), f\"{entity}_trfm.csv\")\r\n",
							"            #dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+entity+'eoy_trfm.csv'\r\n",
							"            #dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"            dest_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"\r\n",
							"            \r\n",
							"            data = pd.DataFrame(_data)\r\n",
							"            #data.to_csv(dest_path,index=False,storage_options = {'account_key':'Wl+cozJ6xDQtnstjWpmHpz/2UGxgaptMpDl1/OMPiLUQS+2fuNdeqMmnvzgCTXTwL7c3oNYjrRvl+AStI26Zaw=='})\r\n",
							"            data.to_csv(dest_path,index=False,storage_options = {'account_key':storageaccountkey})\r\n",
							"            \r\n",
							"            #return dest_path  // this is returned to run function from where tranform() was called\r\n",
							"        \r\n",
							"        #else:\r\n",
							"            #return None\r\n",
							"\r\n",
							"    geo_type = {2:'city',1:'region',0:'country'}\r\n",
							"    geo_prefix = {2:'CI',1:'RE',0:'CO'}\r\n",
							"\r\n",
							"    def transform_geo(self, geo_data):\r\n",
							"        # input - geo_data - list Order - country_code,region,city \r\n",
							"    \r\n",
							"        inc_id = []\r\n",
							"        inc_geo_nodes = []\r\n",
							"        for idx, ge in enumerate(geo_data):\r\n",
							"            if ge:\r\n",
							"                \r\n",
							"                raw_ge = re.sub(r'\\W','',ge)\r\n",
							"                inc_id.append(self.geo_prefix[idx])\r\n",
							"                inc_id.append(raw_ge[0:30].upper())\r\n",
							"                \r\n",
							"                inc_geo_nodes.append(f\"{''.join(inc_id[0:-2])};{''.join(inc_id)};{self.geo_type[idx]};{ge.strip()};{raw_ge.lower()}\")\r\n",
							"                \r\n",
							"        return pd.Series([''.join(inc_id), inc_geo_nodes[-1], '|'.join(inc_geo_nodes)])\r\n",
							"    \r\n",
							"    def ey_entity_check(self, val):\r\n",
							"        return 1 if val in self.config['crunchbase.ey_uuid'] else 0\r\n",
							"    \r\n",
							"    def normalize(self, val):\r\n",
							"        return re.sub(r\"[^a-z 0-9]\",\"\",val.lower()).strip() if val else ''\r\n",
							"    \r\n",
							"    def remove_unknowns(self, val):\r\n",
							"        return None if str(val).lower().strip() == 'unknown' else val\r\n",
							"\r\n",
							"    def remove_spl(self, val):\r\n",
							"        if (str(val) != 'nan') and ( str(val) != 'None'):\r\n",
							"            s = re.sub(r\"[^a-z 0-9]\",\"\",val.lower()).strip()\r\n",
							"        else:\r\n",
							"            s = None\r\n",
							"        return s\r\n",
							"\r\n",
							"    def link_preprocessing(self, val, threshold):\r\n",
							"        if (str(val) != 'nan') and ( str(val) != 'None'):\r\n",
							"            if val[-1] == '/':\r\n",
							"                ret = val[:-1]\r\n",
							"            spltval = val.split('/')\r\n",
							"            i = 0\r\n",
							"            retV = \"\"\r\n",
							"            if len(spltval) >= (threshold):\r\n",
							"                while i<threshold:\r\n",
							"                    retV = retV + spltval[i] + \"/\"\r\n",
							"                    i = i+1\r\n",
							"                retV = retV[:-1]\r\n",
							"        else:\r\n",
							"            retV = None\r\n",
							"        return pd.Series([retV])\r\n",
							"    \r\n",
							"    def get_value(self, file_name: str, lookup_field: str, searched_field: str, lookup_value, default):\r\n",
							"        source = self.config['source_flag']\r\n",
							"        lookup_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"lookup_files\",f\"{source}_{file_name}\")\r\n",
							"        try:\r\n",
							"            lookup_df = pd.read_csv(lookup_file_path, dtype=str)\r\n",
							"        except pd.errors.ParserError as e:\r\n",
							"            self._logger.warning(f\"{str(e)}, Attempting advanced parsing\")\r\n",
							"            lookup_df = pd.read_csv(lookup_file_path, engine='python', quoting=csv.QUOTE_NONE, dtype=str)\r\n",
							"        candidates = lookup_df[(lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        if not candidates.empty:\r\n",
							"            for elem in candidates:\r\n",
							"                value = elem\r\n",
							"                break\r\n",
							"            return value\r\n",
							"        else:\r\n",
							"            return default\r\n",
							"\r\n",
							"    def get_values(self, file_name: str, lookup_field: str, searched_field: list, lookup_value, source_match=False, default=\"\"):\r\n",
							"        source = self.config['source_flag']\r\n",
							"        \r\n",
							"        #file_name = 'org_key_lookup.csv'\r\n",
							"       \r\n",
							"        #file_path2 = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"        file_path2 = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"\r\n",
							"        #lookup_df = cache[file_name] if cache else None\r\n",
							"        if 1==1:\r\n",
							"\r\n",
							"            self._logger.info(f\"{file_name} - Reading from file\")\r\n",
							"\r\n",
							"            #lookup_dir = self.config['storage.lookup_dir']\r\n",
							"            #df_data = DataLakeAdapter(self.config, self._logger).download_file_blob(file_name=f\"{source}_{file_name}\", file_path=lookup_dir)\r\n",
							"            #lookup_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"lookup_files\",f\"{source}_{file_name}\")\r\n",
							"            \r\n",
							"            try:\r\n",
							"                lookup_df = pd.read_csv(file_path2, dtype=str)\r\n",
							"            except pd.errors.ParserError as e:\r\n",
							"                self._logger.warning(f\"{str(e)}, Attempting advanced parsing\")\r\n",
							"                lookup_df = pd.read_csv(file_path2, quoting=csv.QUOTE_NONE, dtype=str)\r\n",
							"            \r\n",
							"            # Update Cache\r\n",
							"            #if cache and file_name in cache:\r\n",
							"                #cache[file_name] = lookup_df \r\n",
							"        \r\n",
							"        #else:\r\n",
							"            #self._logger.info(f\"{file_name} - Reading from in-memory cache - {cache[file_name].shape}\")\r\n",
							"\r\n",
							"        if source_match:\r\n",
							"            candidates = lookup_df[(lookup_df['source'] == source) & (lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        else:\r\n",
							"            candidates = lookup_df[(lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        \r\n",
							"        value = default\r\n",
							"        if not candidates.empty:\r\n",
							"            if isinstance(searched_field, list):\r\n",
							"                for idx, elem in candidates.iterrows():\r\n",
							"                    res = []\r\n",
							"                    for sf in searched_field:\r\n",
							"                        res.append(elem[sf])\r\n",
							"                    value = ';'.join(res)\r\n",
							"                    break\r\n",
							"            else:\r\n",
							"                for elem in candidates:\r\n",
							"                    value = elem\r\n",
							"\r\n",
							"                    break\r\n",
							"                    \r\n",
							"        return value\r\n",
							"\r\n",
							"\r\n",
							"    # Generate org_key\r\n",
							"    def get_org_key(self, source_id):\r\n",
							"        \r\n",
							"        source = self.config['source_flag']\r\n",
							"    \r\n",
							"\r\n",
							"        return self.get_values(file_name=\"org_key_lookup.csv\", \r\n",
							"                            lookup_field=\"source_id\", \r\n",
							"                            searched_field=\"key\", \r\n",
							"                            lookup_value=source_id, \r\n",
							"                            source_match=True, default=source + str(source_id))\r\n",
							"    \r\n",
							"    # Generate emp_key\r\n",
							"    def get_emp_key(self, source_id):\r\n",
							"        \r\n",
							"        source = self.config['source_flag']\r\n",
							"        \r\n",
							"        return self.get_values(file_name=\"emp_key_lookup.csv\", \r\n",
							"                            lookup_field=\"source_id\", \r\n",
							"                            searched_field=\"key\", \r\n",
							"                            lookup_value=source_id, \r\n",
							"                            source_match=True, default=source + str(source_id))\r\n",
							"        \r\n",
							"\r\n",
							"    def get_table_values(self,partition_key,row_key,columns):\r\n",
							"        try:\r\n",
							"            result_query = TableConnector(self.config).search(partition_key=partition_key,row_key=row_key,columns=columns)\r\n",
							"\r\n",
							"            lst = []\r\n",
							"            for col in columns:\r\n",
							"                lst.append(result_query.get(col,\"\"))\r\n",
							"\r\n",
							"            result = \";\".join(lst)\r\n",
							"\r\n",
							"\r\n",
							"        except Exception as err:\r\n",
							"            raise Exception('Error in extracting entities from azure table:{}'.format(err))\r\n",
							"\r\n",
							"        return result\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"entity= 'EOY'\r\n",
							"dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"print(dest_path)"
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import logging\r\n",
							"# logger config\r\n",
							"# log levels: debug, info, warning, error, critical\r\n",
							"logFormatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s',datefmt='%Y-%m-%d %H:%M:%S')\r\n",
							"logger = logging.getLogger('EOY')\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"# stream logger\r\n",
							"sh = logging.StreamHandler() # stream handler\r\n",
							"sh.setFormatter(logFormatter)\r\n",
							"sh.setLevel(logging.INFO)\r\n",
							"logger.addHandler(sh)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"obj = TransformationController(config,logger)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transformations =  _transformation_lookup.get('mappings')\r\n",
							"print(transformations)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/transformation_controller"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run EOYconfig_Update"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"\r\n",
							"#from ce_pipelines import DataLakeAdapter\r\n",
							"#from ce_pipelines.eoy import config,logger\r\n",
							"\r\n",
							"#from .transformations import Transformations\r\n",
							"\r\n",
							"def execute(entities=['eoy'], **kwargs):\r\n",
							"\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    for entity in entities:\r\n",
							"    \r\n",
							"        logger.info(f\"Transformation - {entity} - Started.\")\r\n",
							"        file_name = f'{entity}.csv'\r\n",
							"        local_temp_file_paths = []\r\n",
							"\r\n",
							"        data_lake_conn = DataLakeAdapter(config=config, logger=logger, env='landing', entity=entity)\r\n",
							"        downloaded_file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/staging/eoy'\r\n",
							"        if downloaded_file_path:\r\n",
							"\r\n",
							"            local_temp_file_paths.append(downloaded_file_path)\r\n",
							"            logger.info(f\"Download completed; File Location: {downloaded_file_path}\")\r\n",
							"\r\n",
							"            transformed_file_path = Transformations(config=config, logger=logger).transform(downloaded_file_path, entity) \r\n",
							"            if transformed_file_path:\r\n",
							"                \r\n",
							"                local_temp_file_paths.append(transformed_file_path)\r\n",
							"                logger.info(f\"Transformation completed; File Location: {transformed_file_path}\")\r\n",
							"                data_lake_conn.split_upload_file_from_local(transformed_file_path, entity, env='landing')\r\n",
							"        \r\n",
							"        # Delete temporary files\r\n",
							"        for local_temp_file_path in local_temp_file_paths:\r\n",
							"            if os.path.exists(local_temp_file_path):\r\n",
							"                os.remove(local_temp_file_path)\r\n",
							"                logger.info(f\"File deleted - {local_temp_file_path}\")\r\n",
							"            else:\r\n",
							"                logger.info(f\"File doesn't exist - {local_temp_file_path}\")\r\n",
							"\r\n",
							"        logger.info(f\"Transformation - {entity} - Completed.\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#transformation_controller line number 141, we can pass file path of org_key_lookup.csv from def get_org_key() in get_values() and \r\n",
							"#then we can create datafrae from using that file path in get_values().\r\n",
							"\r\n",
							"#df_data = DataLakeAdapter(self.config, self._logger).download_file_blob(file_name=f\"{source}_{file_name}\", file_path=lookup_dir)\r\n",
							"df_data = pd.read_csv()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Transpy')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ee5dff1e-fd3c-4cce-924b-9aa742cc3235"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run secondCntrlrNB"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def mul(config, x, y):\r\n",
							"    print(\"Transpy\")\r\n",
							"    z = x*y\r\n",
							"    return z\r\n",
							"\r\n",
							"\r\n",
							"def mul_Transpy(val):\r\n",
							"    x=val[0]\r\n",
							"    y=val[1]\r\n",
							"    print(\"Transpy called from Controller\")\r\n",
							"    print(\"called from Controller NB: \",config[\"source_flag\"])\r\n",
							"    z = x*y\r\n",
							"    return z"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/archive')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9a458a32-c60f-49f9-8f16-95f5c51b5064"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"from datetime import datetime\r\n",
							"from azure.storage.filedatalake import DataLakeServiceClient\r\n",
							"\r\n",
							"class Archive():\r\n",
							"\r\n",
							"    def __init__(self,config,entity=None):\r\n",
							"        self._config = config\r\n",
							"        self._entity = entity\r\n",
							"\r\n",
							"\r\n",
							"    def move_folder(self):\r\n",
							"        container_name = self._config.get(\"storage.container_name\")\r\n",
							"        url = \"{}://{}.dfs.core.windows.net\".format(\"https\", self._config.get(\"storage.account_name\"))\r\n",
							"        try:\r\n",
							"            # Get today's datetime\r\n",
							"            today = str(datetime.now())\r\n",
							"            file_system_client = DataLakeServiceClient(account_url=url,credential=self._config.get(\"storage.account_key\")).get_file_system_client(\r\n",
							"                                                                                                                file_system=container_name)\r\n",
							"\r\n",
							"            # if archive flag is True, then move the files to archive directory\r\n",
							"            if self._config.get(\"archive_flag\"):\r\n",
							"                # create a directory with today's date\r\n",
							"                file_system_client.create_directory(\"archive/\" + today)\r\n",
							"\r\n",
							"                # create a directory with entity name\r\n",
							"                file_system_client.create_directory(\"archive/\" + today +  \"/\" + self._entity.lower())\r\n",
							"\r\n",
							"                # # check whether this directory already had any files in it and delete them\r\n",
							"                # file_path_entity = file_system_client.get_paths(path=\"archive/\" + today + \"/\" + self._entity)\r\n",
							"                # for path in file_path_entity:\r\n",
							"                #     file_system_client.delete_file(path.name)\r\n",
							"\r\n",
							"                # moving files from landing to archive directory\r\n",
							"                file_paths = file_system_client.get_paths(path=\"landing/\" + self._entity.lower(), recursive=False)\r\n",
							"                for path in file_paths:\r\n",
							"                    file_name_tokens = path.name.split('/')\r\n",
							"                    file_name = ('/').join(file_name_tokens[1:])\r\n",
							"                    directory_client = file_system_client.get_directory_client(path.name)\r\n",
							"                    new_dir_name = \"archive/\" + today + \"/\" + file_name\r\n",
							"                    directory_client.rename_directory(new_name=directory_client.file_system_name + '/' + new_dir_name)\r\n",
							"\r\n",
							"            # if archive flag is False, delete the files from landing directory\r\n",
							"            else:\r\n",
							"                directory_path = file_system_client.get_paths(path=\"landing/\" + self._entity.lower(), recursive=False)\r\n",
							"                for path in directory_path:\r\n",
							"                    inner_directory_path = file_system_client.get_paths(path=path.name, recursive=False)\r\n",
							"                    for inner_path in inner_directory_path:\r\n",
							"                        file_system_client.delete_file(inner_path.name, recursive=False)\r\n",
							"\r\n",
							"                        try:\r\n",
							"                            file_system_client.delete_directory(path.name, recursive=False)\r\n",
							"                        except:\r\n",
							"                            pass\r\n",
							"\r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            raise Exception(e)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/az_table_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4cdbe3db-907b-487b-a82f-025e4787084d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install azure-data-tables"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.data.tables import TableClient\r\n",
							"\r\n",
							"class TableConnector:\r\n",
							"    def __init__(self,config):\r\n",
							"        self.config = config\r\n",
							"\r\n",
							"        # Connect to Azure table\r\n",
							"        connection_str = \"DefaultEndpointsProtocol=https;AccountName=\" + self.config.get(\r\n",
							"            'storage.account_name') + \";AccountKey=\" + \\\r\n",
							"                         self.config.get('storage.account_key') + \";EndpointSuffix=core.windows.net\"\r\n",
							"\r\n",
							"        self._table_service = TableClient.from_connection_string(connection_str,\r\n",
							"                                                           table_name=self.config.get('storage.table_name'))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"    def search(self, partition_key, row_key,columns):\r\n",
							"        filter = \"PartitionKey eq '{}' and RowKey eq '{}'\".format(partition_key,row_key)\r\n",
							"\r\n",
							"        query_result = self._table_service.query_entities(query_filter=filter, select=columns)\r\n",
							"\r\n",
							"\r\n",
							"        result = {}\r\n",
							"        for query in query_result:\r\n",
							"            for col_names in columns:\r\n",
							"                result[col_names] = query[col_names]\r\n",
							"\r\n",
							"        return  result\r\n",
							""
						],
						"outputs": [],
						"execution_count": 37
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c9315a3b-ec7e-4dfb-953a-3d97b1127f9d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"config = {\r\n",
							"\r\n",
							"\r\n",
							"#    \"storage.account_name\":\"bmdatalakestorage2023\",\r\n",
							"#    \"storage.account_key\":\"Wl+cozJ6xDQtnstjWpmHpz/2UGxgaptMpDl1/OMPiLUQS+2fuNdeqMmnvzgCTXTwL7c3oNYjrRvl+AStI26Zaw==\",\r\n",
							"#    \"storage.container_name\":\"test\",\r\n",
							"    \"storage.lookup_dir\":\"data/lookup_data\",\r\n",
							"    \"storage.temp_dir\":\"data/temp_data\",\r\n",
							"\r\n",
							"    \"storage.account_name\":\"ehub-secret-storage-account-name\",\r\n",
							"    \"storage.account_key\":\"ehub-secret-storage-key\",\r\n",
							"    \"storage.container_name\":\"ehub-secret-adls-container-name\",\r\n",
							"\r\n",
							"    \"keyvault.name\":\"bmehubkeyvault2023\",\r\n",
							"    \"linkedservice.keyvaultls\":\"ls_bmehubkv2023\",\r\n",
							"\r\n",
							"    \"logging.table_name\":\"ETLlogging\",\r\n",
							"\r\n",
							"    #\"neo4j.bolt_url\": \"bolt://54.196.135.240:7687\",\r\n",
							"    #\"neo4j.user_name\": \"neo4j\",\r\n",
							"    #\"neo4j.password\": \"diesel-velocities-originals\",\r\n",
							"    #\"neo4j.database\": \"neo4j\",\r\n",
							"\r\n",
							"    \"crunchbase_uri\": \"https://api.crunchbase.com/bulk/premium/v4/bulk_export.tar.gz?user_key=003b629a937369811e06cf5c1ae4aac3\"\r\n",
							"}\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/crun_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/crunchbase/crunchbase_config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f7630ea3-d092-48c6-a39a-3f3ff2111adf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"com_config = config"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"config = {\r\n",
							"    \r\n",
							"    \"source_flag\": \"CB\",\r\n",
							"    \"crunchbase_uri\": \"https://api.crunchbase.com/bulk/premium/v4/bulk_export.tar.gz?user_key=003b629a937369811e06cf5c1ae4aac3\",\r\n",
							"    \"entity_files\": [\"organizations\", \"funding_rounds\", \"investments\", \"org_parents\", \"acquisitions\", \"people\", \"people_descriptions\", \"degrees\", \"jobs\"],\r\n",
							"    \"datetime_format\": \"%Y-%m-%d %H:%M:%S\",\r\n",
							"    \r\n",
							"    \"storage.staging_dir_name\":\"staging/crunchbase\",\r\n",
							"    \"storage.chunksize\":100,\r\n",
							"    \"storage.landing_dir_name\":\"landing/crunchbase\",\r\n",
							"\r\n",
							"    \"data.chunksize\":10000,\r\n",
							"    \"crunchbase.ey_uuid\": [\"a325d318-5167-525b-f61d-2ab89d6b05c9\"],\r\n",
							"\r\n",
							"    \"transformations\": {\r\n",
							"        \r\n",
							"        \"organizations_inc\":{\r\n",
							"            \"invalid_columns\":[\"permalink\", \"roles\", \"state_code\", \"category_list\", \"category_groups_list\", \"num_funding_rounds\", \"total_funding\", \"total_funding_currency_code\", \"closed_on\", \"alias1\", \"alias2\", \"alias3\"], \r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"startup_flag\",\r\n",
							"                    \"src_column_names\": [\"revenue_range\",\"employee_count\",\"status\",\"last_funding_on\",\"founded_on\"],\r\n",
							"                    \"dst_column_names\": \"startup_flag\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"get_org_key\",\r\n",
							"                    \"src_column_names\": \"uuid\",\r\n",
							"                    \"dst_column_names\": \"org_key\"\r\n",
							"                },            \r\n",
							"                {\r\n",
							"                    \"function\": \"handle_categories\",\r\n",
							"                    \"src_column_names\": [\"category_list\",\"category_groups_list\"],\r\n",
							"                    \"dst_column_names\": \"categories\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"handle_employee_count\",\r\n",
							"                    \"src_column_names\": \"employee_count\",\r\n",
							"                    \"dst_column_names\": \"employee_count\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"identify_academy\",\r\n",
							"                    \"src_column_names\": ['domain', 'roles'],\r\n",
							"                    \"dst_column_names\": \"role\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"identify_entity\",\r\n",
							"                    \"src_column_names\": ['uuid', 'roles', 'num_exits'],\r\n",
							"                    \"dst_column_names\": \"type\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"ey_entity_check\",\r\n",
							"                    \"src_column_names\": \"uuid\",\r\n",
							"                    \"dst_column_names\": \"is_ey\"\r\n",
							"                }\r\n",
							"                \r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"organizations_geo_inc\": {\r\n",
							"            \"invalid_columns\":[\"country_code\",\"region\",\"city\"],\r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"transform_geo\",\r\n",
							"                    \"src_column_names\": ['country_code','region','city'],\r\n",
							"                    \"dst_column_names\": ['id', 'geo_entity', 'geo_path']\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"people_inc\":{\r\n",
							"            \r\n",
							"            \"invalid_columns\":[\"type\",\"permalink\",\"state_code\"],\r\n",
							"            \r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"get_emp_key\",\r\n",
							"                    \"src_column_names\": \"uuid\",\r\n",
							"                    \"dst_column_names\": \"per_key\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"ey_entity_check\",\r\n",
							"                    \"src_column_names\": \"featured_job_organization_uuid\",\r\n",
							"                    \"dst_column_names\": \"is_ey\"\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"people_geo_inc\": {\r\n",
							"            \"invalid_columns\":[\"country_code\",\"region\",\"city\"],\r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"transform_geo\",\r\n",
							"                    \"src_column_names\": ['country_code','region','city'],\r\n",
							"                    \"dst_column_names\": ['id', 'geo_entity', 'geo_path']\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"funding_rounds_inc\":{\r\n",
							"            \"invalid_columns\":[\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"country_code\",\"state_code\",\"region\",\"city\",\"raised_amount\",\r\n",
							"                \"raised_amount_currency_code\",\"post_money_valuation_usd\",\"post_money_valuation\",\"post_money_valuation_currency_code\",\"investor_count\"],\r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"handle_investment_type\",\r\n",
							"                    \"src_column_names\": \"investment_type\",\r\n",
							"                    \"dst_column_names\": \"investment_type\"\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"degrees_inc\":{\r\n",
							"            \r\n",
							"            \"invalid_columns\":[\"uuid\",\"name\",\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"],\r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"remove_unknowns\",\r\n",
							"                    \"src_column_names\": \"subject\",\r\n",
							"                    \"dst_column_names\": \"subject\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"remove_unknowns\",\r\n",
							"                    \"src_column_names\": \"degree_type\",\r\n",
							"                    \"dst_column_names\": \"degree_type\"\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        },\r\n",
							"        \"acquisitions_inc\":{\r\n",
							"            \"invalid_columns\":[\"uuid\",\"name\",\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\",\"acquiree_cb_url\",\r\n",
							"                \"acquiree_country_code\",\"acquiree_state_code\",\"acquiree_region\",\"acquiree_city\",\"acquirer_cb_url\",\r\n",
							"                \"acquirer_country_code\",\"acquirer_state_code\",\"acquirer_region\",\"acquirer_city\",\"price\",\"price_currency_code\"]\r\n",
							"        },\r\n",
							"        \"investments_organizations_inc\":{\r\n",
							"            \"invalid_columns\":[\"uuid\",\"name\",\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"]\r\n",
							"        },\r\n",
							"        \"investments_people_inc\":{\r\n",
							"            \"invalid_columns\":[\"uuid\",\"name\",\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"]\r\n",
							"        },\r\n",
							"        \"jobs_inc\":{\r\n",
							"            \"invalid_columns\":[\"uuid\",\"name\",\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"]\r\n",
							"        },\r\n",
							"        \"org_parents_inc\":{\r\n",
							"            \"invalid_columns\":[\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"]\r\n",
							"        },\r\n",
							"        \"people_descriptions_inc\":{\r\n",
							"            \"invalid_columns\":[\"type\",\"permalink\",\"cb_url\",\"rank\",\"created_at\",\"updated_at\"]\r\n",
							"        }\r\n",
							"        \r\n",
							"    }\r\n",
							"}\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"config.update(com_config)\r\n",
							"print(config)"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/crunch_run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/crunchbase/extraction_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "501852ca-65ba-4930-ac84-d7c53ce411ea"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/etl_status_recorder"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/crunchbase/crunchbase_config/crun_config"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/crunchbase/crunchbase_config/logger_config"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-data-tables"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/ETL_Logger"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/crunchbase/extraction_operator/src_downloader"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/crunchbase/extraction_operator/organizations/extractor"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import tempfile\r\n",
							"import os\r\n",
							"import shutil\r\n",
							"import datetime\r\n",
							"\r\n",
							"#from ce_pipelines import DataLakeAdapter\r\n",
							"#from ce_pipelines import ETLStatusRecorder\r\n",
							"#from ce_pipelines.crunchbase import config, logger\r\n",
							"#from ce_pipelines import ETLlogger\r\n",
							"\r\n",
							"#from .source_downloader import DataDownloader \r\n",
							"#from .organizations import OrganizationsDataExtractor\r\n",
							"\r\n",
							"\r\n",
							"def execute(**kwargs):\r\n",
							"    print(\"bcjjnk\")\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"\r\n",
							"    #Last execution date\r\n",
							"    esr = ETLStatusRecorder(config, logger)\r\n",
							"    last_successfully_executed_at = esr.get_last_successful_execution_at()\r\n",
							"    print(\"hi ji\")\r\n",
							"\r\n",
							"    if last_successfully_executed_at:\r\n",
							"        config.update({'start_date': last_successfully_executed_at})\r\n",
							"\r\n",
							"    # Temp dir created for data download\r\n",
							"    dir_path = tempfile.mkdtemp()\r\n",
							"    logger.info(f\"Temp dir created - {dir_path}\")\r\n",
							"    \r\n",
							"    # Initialize Staging Location\r\n",
							"    staging = DataLakeAdapter(config=config, logger=logger, env='staging')\r\n",
							"\r\n",
							"    # Download Crunchbase daily data export\r\n",
							"    DataDownloader(config=config, logger=logger).download_data(dir_path)\r\n",
							"    organizations = OrganizationsDataExtractor(staging, config=config, logger=logger).extract(dir_path)\r\n",
							"    \r\n",
							"    # Delete temp directory after incremental data staging\r\n",
							"    shutil.rmtree(dir_path)\r\n",
							"    logger.info(f\"Temp dir deleted - {dir_path}\")\r\n",
							"\r\n",
							"    # Update logs to Azure Table\r\n",
							"    etl_logger = ETLlogger(_config=config)\r\n",
							"    etl_logger.update_logs(task=\"Extraction\", result=\"Success\")\r\n",
							"\r\n",
							"#if __name__ == \"__main__\":\r\n",
							"    \r\n",
							"    '''\r\n",
							"    Optional Arguments:\r\n",
							"    --------------------\r\n",
							"    1. crunchbase_uri - crunchbase download uri\r\n",
							"    2. entity_files - list of files to be extracted from crunchbase archive\r\n",
							"    3. start_date - datetime(%Y-%m-%d %H:%M:%S) filter for records  \r\n",
							"    4. end_date - datetime(%Y-%m-%d %H:%M:%S) filter for records\r\n",
							"    '''\r\n",
							"    \r\n",
							"    execute(start_date=\"2017-01-01 00:00:00\", end_date=\"2020-03-30 00:00:00\")\r\n",
							"\r\n",
							"\r\n",
							"def check_date_formats(dates_list):\r\n",
							"\r\n",
							"    if len(dates_list) < 2:\r\n",
							"        raise ValueError(\"Start date and end date have to be declared!\")\r\n",
							"\r\n",
							"    date_format = config['datetime_format']\r\n",
							"    \r\n",
							"    for date_text in dates_list[:2]:\r\n",
							"        try:\r\n",
							"            datetime.datetime.strptime(date_text, date_format)\r\n",
							"            print(\"Date {date_text} is ok\".format(date_text=date_text))\r\n",
							"        except ValueError:\r\n",
							"            raise ValueError(\"Incorrect date {date_text}, format should be {date_format}\"\r\n",
							"                            .format(date_text=date_text, date_format=date_format))\r\n",
							"    \r\n",
							"    return { \"start_date\": dates_list[0], \"end_date\": dates_list[1] }\r\n",
							""
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute()"
						],
						"outputs": [],
						"execution_count": 41
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/crunchbaseAPI')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "11fccfae-2fbe-43d6-8fff-4d25e049b0c9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import requests \r\n",
							"import tarfile\r\n",
							"from io import BytesIO\r\n",
							"response = requests.get(\"https://api.crunchbase.com/bulk/premium/v4/bulk_export.tar.gz?user_key=003b629a937369811e06cf5c1ae4aac3\")\r\n",
							"if response.status_code != 200:\r\n",
							"    print(\"Crunchbase Daily Data Export download failed\")\r\n",
							"else:\r\n",
							"    print(\"Crunchbase Daily Data Export download completed\")\r\n",
							"data = BytesIO(response.content)\r\n",
							"with tarfile.open(fileobj=data) as tar:\r\n",
							"    members = tar.getmember('organizations' + '.csv')  \r\n",
							"    #tar.extractall(dir_path, members=members)\r\n",
							"    data.close()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install tarfile\r\n",
							"pip install requests"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datalake_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cffbbd56-ca0e-4f30-bcd9-fd498f46e7e1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.filedatalake import DataLakeServiceClient\r\n",
							"from azure.storage.blob import generate_account_sas, ResourceTypes, AccountSasPermissions"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"class DataLakeAdapter:\r\n",
							"    \r\n",
							"    def __init__(self, config, logger, env=None, entity=None):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        self._storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        self._storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        self._storage_url = f\"https://{self._storageaccountname}.dfs.core.windows.net\"\r\n",
							"    #     if env:\r\n",
							"    #         self._init_directory_structure(env, entity)\r\n",
							"\r\n",
							"    # def _init_directory_structure(self, env, entity=None):\r\n",
							"    #     file_system_client = DataLakeServiceClient(\r\n",
							"    #         account_url=self._storage_url, \r\n",
							"    #         credential=self._storage_credential\r\n",
							"    #     ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"    #     dir_path=self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else '')\r\n",
							"\r\n",
							"    #     try:\r\n",
							"    #         file_system_client.delete_directory(dir_path)\r\n",
							"    #     except Exception as e:\r\n",
							"    #         if 'PathNotFound' in str(e):\r\n",
							"    #             #self._logger.info(f'Creating new dir {dir_path}')\r\n",
							"    #             print(\"Creating new dir\")\r\n",
							"    #         else:\r\n",
							"    #             print(\"Delete dir\")\r\n",
							"    #             #self._logger.error(f'Delete dir {dir_path} failed with exception, {e}')\r\n",
							"    #             #raise e\r\n",
							"\r\n",
							"    #     file_system_client.create_directory(dir_path)\r\n",
							"    #     file_system_client.close()\r\n",
							"    #     #self._logger.info(f'New dir created {dir_path}')\r\n",
							"\r\n",
							"    def upload_file(self, file_name, file_data, env, entity=None):\r\n",
							"        file_system_client = DataLakeServiceClient(\r\n",
							"            account_url=self._storage_url, \r\n",
							"            credential=self._storage_credential\r\n",
							"        ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"        dir_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else ''))\r\n",
							"        file_client = dir_client.get_file_client(file_name)\r\n",
							"        file_data.seek(0)\r\n",
							"        file_client.upload_data(file_data.getvalue(), overwrite=True)\r\n",
							"        file_client.close()\r\n",
							"        dir_client.close()\r\n",
							"        file_system_client.close()\r\n",
							"        #self._logger.info(f'File upload completed, {file_name}') \r\n",
							"\r\n",
							"    def download_file_blob(self, file_name, file_path):\r\n",
							"        file_system_client = DataLakeServiceClient(\r\n",
							"            account_url=self._storage_url, \r\n",
							"            credential=self._storage_credential\r\n",
							"        ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"        dir_client = file_system_client.get_directory_client(file_path)\r\n",
							"        file_client = dir_client.get_file_client(file_name)\r\n",
							"        file_hndlr = file_client.download_file()\r\n",
							"        file_data = file_hndlr.readall()\r\n",
							"        #self._logger.info(f'File download completed, {file_name}') \r\n",
							"        return file_data\r\n",
							"\r\n",
							"    def split_upload_file_from_local(self, source_file_path, entity, env='landing'):\r\n",
							"        try:\r\n",
							"            #source_file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"            logger.info(f\"Split Transformation file start; File Location: {source_file_path}\")\r\n",
							"            chunksize = self._config[\"storage.chunksize\"]\r\n",
							"            fileuploadname = 'part-'\r\n",
							"            data_chunks  = pd.read_csv(source_file_path, chunksize=chunksize, dtype=str)\r\n",
							"            _file_cnt = 0\r\n",
							"            for chunks in data_chunks:\r\n",
							"                logger.info(f\"File Chunk number: {_file_cnt}\")\r\n",
							"                data = pd.DataFrame(chunks)\r\n",
							"                uploadFileName = fileuploadname + str(_file_cnt).rjust(5,'0')\r\n",
							"\r\n",
							"                #landing_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]+'/'+entity\r\n",
							"                landing_path = 'abfss://'+self._storagecontainername+'@'+self._storageaccountname+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]+'/'+entity+'/'+uploadFileName+'.csv'\r\n",
							"                logger.info(f\"Split transformed file path: {landing_path}\")\r\n",
							"                data.to_csv(landing_path, index=False)\r\n",
							"            \r\n",
							"                # self.upload_file(f'{uploadFileName}.csv', data, env, entity=entity)\r\n",
							"                \r\n",
							"                _file_cnt+=1\r\n",
							"\r\n",
							"            self._logger.info(f'File Split upload completed for entity - {entity}; No. of parts {_file_cnt}') \r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(e)\r\n",
							"            return None   \r\n",
							"\r\n",
							"    def download_file(self, file_name, dest_dir_path=tempfile.gettempdir(), env=None):\r\n",
							"        try:\r\n",
							"            file_system_client = DataLakeServiceClient(\r\n",
							"                account_url=self._storage_url, \r\n",
							"                credential=self._storage_credential\r\n",
							"            ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"            directory_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"])\r\n",
							"            file_client = directory_client.get_file_client(file_name)\r\n",
							"            download = file_client.download_file()\r\n",
							"            downloaded_bytes = download.readall()\r\n",
							"            file_path = os.path.join(dest_dir_path, file_name)\r\n",
							"            local_file = open(file_path,'wb')\r\n",
							"            local_file.write(downloaded_bytes)\r\n",
							"            local_file.close()\r\n",
							"            #self._logger.info(f'File download completed, {file_name}') \r\n",
							"            return file_path\r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            return None\r\n",
							"\r\n",
							"    def get_files(self, env, entity):\r\n",
							"        self._logger.info(f\"get_fiiles started:\")\r\n",
							"        try:\r\n",
							"            file_system_client = DataLakeServiceClient(\r\n",
							"                account_url=self._storage_url, \r\n",
							"                credential=self._storageaccountkey\r\n",
							"            ).get_file_system_client(file_system=self._storagecontainername)\r\n",
							"\r\n",
							"            env_path = self._config[f\"storage.{env}_dir_name\"]\r\n",
							"            self._logger.info(f\"env_path: {env_path}\")\r\n",
							"            path1 = f\"{env_path}/{entity}\"\r\n",
							"            self._logger.info(f\"path1: {path1}\")\r\n",
							"            paths = file_system_client.get_paths(path=f\"{env_path}/{entity}\")\r\n",
							"            return [p.name for p in paths]\r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(e)\r\n",
							"            return \"0\"\r\n",
							"            \r\n",
							"                \r\n",
							"    def generate_sas_url(self, file_path):\r\n",
							"        self._logger.info(f\"Generating SAS for file_path: {file_path}\")\r\n",
							"        try:\r\n",
							"            sas_token = generate_account_sas(\r\n",
							"                account_name=self._storageaccountname,\r\n",
							"                account_key=self._storageaccountkey,\r\n",
							"                resource_types=ResourceTypes(object=True,),\r\n",
							"                permission=AccountSasPermissions(read=True),\r\n",
							"                expiry=datetime.utcnow() + timedelta(hours=1)\r\n",
							"            )\r\n",
							"\r\n",
							"            return f\"https://{self._storageaccountname}.blob.core.windows.net/{self._storagecontainername}/{file_path}?{sas_token}\"\r\n",
							"        \r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(e)"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datalake_connector_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "fe7b19bc-5d7c-451e-80b4-7ab3fcffd296"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.filedatalake import DataLakeServiceClient\r\n",
							"from azure.storage.blob import generate_account_sas, ResourceTypes, AccountSasPermissions"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# print(config[\"storage.account_name\"])\r\n",
							"# #print(e_config)\r\n",
							"# cm_config = config\r\n",
							"# print(cm_config)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# print(config[\"data.chunksize\"])\r\n",
							"# config.update(cm_config)\r\n",
							"# print(config)\r\n",
							"# print(config[\"storage.account_name\"])\r\n",
							"# print(config[\"storage.lookup_dir\"])"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storageaccountname)\r\n",
							"# storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storagecontainername)\r\n",
							"# storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storageaccountkey)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/logger_config"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class DataLakeAdapter:\r\n",
							"    \r\n",
							"    def __init__(self, config, logger, env=None, entity=None):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        # self._storage_credential = \"Wl+cozJ6xDQtnstjWpmHpz/2UGxgaptMpDl1/OMPiLUQS+2fuNdeqMmnvzgCTXTwL7c3oNYjrRvl+AStI26Zaw==\"\r\n",
							"        # self._storage_container = \"bmfilesystem2023\"\r\n",
							"        self._storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        # print(self.storageaccountname)\r\n",
							"        self._storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        # print(self.storagecontainername)\r\n",
							"        self._storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        # print(self.storageaccountkey)\r\n",
							"        # self._storage_url = f\"https//{self._storageaccountname}.blob.core.windows.net\"\r\n",
							"        self._storage_url = \"https://bmdatalakestorage2023.dfs.core.windows.net\"\r\n",
							"        self._logger.info(f\"DataLakeAdapter _storage_url : {self._storage_url}\")\r\n",
							"    #     if env:\r\n",
							"    #         self._init_directory_structure(env, entity)\r\n",
							"\r\n",
							"    # def _init_directory_structure(self, env, entity=None):\r\n",
							"    #     file_system_client = DataLakeServiceClient(\r\n",
							"    #         account_url=self._storage_url, \r\n",
							"    #         credential=self._storage_credential\r\n",
							"    #     ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"    #     dir_path=self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else '')\r\n",
							"\r\n",
							"    #     try:\r\n",
							"    #         file_system_client.delete_directory(dir_path)\r\n",
							"    #     except Exception as e:\r\n",
							"    #         if 'PathNotFound' in str(e):\r\n",
							"    #             #self._logger.info(f'Creating new dir {dir_path}')\r\n",
							"    #             print(\"Creating new dir\")\r\n",
							"    #         else:\r\n",
							"    #             print(\"Delete dir\")\r\n",
							"    #             #self._logger.error(f'Delete dir {dir_path} failed with exception, {e}')\r\n",
							"    #             #raise e\r\n",
							"\r\n",
							"    #     file_system_client.create_directory(dir_path)\r\n",
							"    #     file_system_client.close()\r\n",
							"    #     #self._logger.info(f'New dir created {dir_path}')\r\n",
							"\r\n",
							"    def upload_file(self, file_name, file_data, env, entity=None):\r\n",
							"        file_system_client = DataLakeServiceClient(\r\n",
							"            account_url=self._storage_url, \r\n",
							"            credential=self._storage_credential\r\n",
							"        ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"        dir_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else ''))\r\n",
							"        file_client = dir_client.get_file_client(file_name)\r\n",
							"        file_data.seek(0)\r\n",
							"        file_client.upload_data(file_data.getvalue(), overwrite=True)\r\n",
							"        file_client.close()\r\n",
							"        dir_client.close()\r\n",
							"        file_system_client.close()\r\n",
							"        #self._logger.info(f'File upload completed, {file_name}') \r\n",
							"\r\n",
							"    def download_file_blob(self, file_name, file_path):\r\n",
							"        file_system_client = DataLakeServiceClient(\r\n",
							"            account_url=self._storage_url, \r\n",
							"            credential=self._storage_credential\r\n",
							"        ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"        dir_client = file_system_client.get_directory_client(file_path)\r\n",
							"        file_client = dir_client.get_file_client(file_name)\r\n",
							"        file_hndlr = file_client.download_file()\r\n",
							"        file_data = file_hndlr.readall()\r\n",
							"        #self._logger.info(f'File download completed, {file_name}') \r\n",
							"        return file_data\r\n",
							"\r\n",
							"    def split_upload_file_from_local(self, source_file_path, entity, env='landing'):\r\n",
							"        try:\r\n",
							"            #source_file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"            logger.info(f\"Split Transformation file start; File Location: {source_file_path}\")\r\n",
							"            chunksize = self._config[\"storage.chunksize\"]\r\n",
							"            fileuploadname = 'part-'\r\n",
							"            data_chunks  = pd.read_csv(source_file_path, chunksize=chunksize, dtype=str)\r\n",
							"            _file_cnt = 0\r\n",
							"            for chunks in data_chunks:\r\n",
							"                logger.info(f\"File Chunk number: {_file_cnt}\")\r\n",
							"                data = pd.DataFrame(chunks)\r\n",
							"                uploadFileName = fileuploadname + str(_file_cnt).rjust(5,'0')\r\n",
							"\r\n",
							"                #landing_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]+'/'+entity\r\n",
							"                landing_path = 'abfss://'+self._storagecontainername+'@'+self._storageaccountname+'.dfs.core.windows.net/'+config[\"storage.landing_dir_name\"]+'/'+entity+'/'+uploadFileName+'.csv'\r\n",
							"                logger.info(f\"Split transformed file path: {landing_path}\")\r\n",
							"                data.to_csv(landing_path, index=False)\r\n",
							"            \r\n",
							"                # self.upload_file(f'{uploadFileName}.csv', data, env, entity=entity)\r\n",
							"                \r\n",
							"                _file_cnt+=1\r\n",
							"\r\n",
							"            self._logger.info(f'File Split upload completed for entity - {entity}; No. of parts {_file_cnt}') \r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(e)\r\n",
							"            return None   \r\n",
							"\r\n",
							"    def download_file(self, file_name, dest_dir_path=tempfile.gettempdir(), env=None):\r\n",
							"        try:\r\n",
							"            file_system_client = DataLakeServiceClient(\r\n",
							"                account_url=self._storage_url, \r\n",
							"                credential=self._storage_credential\r\n",
							"            ).get_file_system_client(file_system=self._storage_container)\r\n",
							"\r\n",
							"            directory_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"])\r\n",
							"            file_client = directory_client.get_file_client(file_name)\r\n",
							"            download = file_client.download_file()\r\n",
							"            downloaded_bytes = download.readall()\r\n",
							"            file_path = os.path.join(dest_dir_path, file_name)\r\n",
							"            local_file = open(file_path,'wb')\r\n",
							"            local_file.write(downloaded_bytes)\r\n",
							"            local_file.close()\r\n",
							"            #self._logger.info(f'File download completed, {file_name}') \r\n",
							"            return file_path\r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            return None\r\n",
							"\r\n",
							"    def get_files(self, env, entity):\r\n",
							"        self._logger.info(f\"get_fiiles started:\")\r\n",
							"        try:\r\n",
							"            file_system_client = DataLakeServiceClient(\r\n",
							"                account_url=self._storage_url, \r\n",
							"                credential=self._storageaccountkey\r\n",
							"            ).get_file_system_client(file_system=self._storagecontainername)\r\n",
							"\r\n",
							"            env_path = self._config[f\"storage.{env}_dir_name\"]\r\n",
							"            self._logger.info(f\"env_path: {env_path}\")\r\n",
							"            path1 = f\"{env_path}/{entity}\"\r\n",
							"            self._logger.info(f\"path1: {path1}\")\r\n",
							"            paths = file_system_client.get_paths(path=f\"{env_path}/{entity}\")\r\n",
							"            return paths\r\n",
							"            # return [p.name for p in paths]\r\n",
							"\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(e)\r\n",
							"            return \"0\"\r\n",
							"            \r\n",
							"                \r\n",
							"    def generate_sas_url(self, file_path):\r\n",
							"        self._logger.info(f\"Generating SAS for file_path: {file_path}\")\r\n",
							"        sas_token = generate_account_sas(\r\n",
							"            account_name=self._storageaccountname,\r\n",
							"            account_key=self._storageaccountkey,\r\n",
							"            resource_types=ResourceTypes(object=True,),\r\n",
							"            permission=AccountSasPermissions(read=True),\r\n",
							"            expiry=datetime.utcnow() + timedelta(hours=1)\r\n",
							"        )\r\n",
							"\r\n",
							"        return f\"https://{self._storageaccountname}.blob.core.windows.net/{self._storagecontainername}/{file_path}?{sas_token}\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"paths = DataLakeAdapter(config=config, logger=logger).get_files(env='landing', entity='eoy')\r\n",
							"for path in paths:\r\n",
							"    print(f\"path = {path}\")"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dummy_class')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/load_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "70abfd37-ecdf-49fa-a6eb-6dc06d83a5d9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/eoy/load_operator/queries/eoy_cql"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class GraphQuery:\r\n",
							"    def getgraphquery(entity):\r\n",
							"        return entity+'_cypher'\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0016a773-397b-48ee-affa-c935fd27e30d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"com_config = config"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"config = {\r\n",
							"    \"source_flag\": \"ENT\",\r\n",
							"    \"datetime_format\": \"%Y-%m-%d %H:%M:%S\",\r\n",
							"        \r\n",
							"    \"storage.staging_dir_name\":\"data/staging/ent\",\r\n",
							"    \"storage.chunksize\":200,\r\n",
							"    \"storage.landing_dir_name\":\"data/landing/ent\",\r\n",
							"    \r\n",
							"    \"data.chunksize\":10000,\r\n",
							"        \r\n",
							"    \"transformations\": {\r\n",
							"        \"ent\":{\r\n",
							"            \"invalid_columns\":[], \r\n",
							"            \"mappings\":[\r\n",
							"                \r\n",
							"                {\r\n",
							"                    \"function\": \"generate_org_id\",\r\n",
							"                    \"src_column_names\": \"company_name\",\r\n",
							"                    \"dst_column_names\": \"org_id\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"generate_per_id\",\r\n",
							"                    \"src_column_names\": [\"entrepreneurs_name\",\"company_name\"],\r\n",
							"                    \"dst_column_names\": \"per_id\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"get_org_key\",\r\n",
							"                    \"src_column_names\": \"org_id\",\r\n",
							"                    \"dst_column_names\": \"org_key\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"get_emp_key\",\r\n",
							"                    \"src_column_names\": \"per_id\",\r\n",
							"                    \"dst_column_names\": \"per_key\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"generate_event_key\",\r\n",
							"                    \"src_column_names\": [\"source\",\"event_name\",\"event_area\",\"event_region\",\"event_year\"],\r\n",
							"                    \"dst_column_names\": \"event_key\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"list_social_media_ids\",\r\n",
							"                    \"src_column_names\": [\"company_social_media\",\"company_username\"],\r\n",
							"                    \"dst_column_names\": \"social_media\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"prep_ey_industry\",\r\n",
							"                    \"src_column_names\": \"ey_industry\",\r\n",
							"                    \"dst_column_names\": \"ey_industry\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"generate_sector_id\",\r\n",
							"                    \"src_column_names\": \"ey_industry\",\r\n",
							"                    \"dst_column_names\": \"sector_id\"\r\n",
							"                }\r\n",
							"                \r\n",
							"            ]\r\n",
							"        }   \r\n",
							"    }\r\n",
							"}\r\n",
							"\r\n",
							"#config.update(base_config)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"config.update(com_config)\r\n",
							"print(config)"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_cql')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/load_operator/queries"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "448b98c2-bd84-4981-a2d9-dab5a2f6ac26"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"ent_cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\"  AS line  \r\n",
							"\r\n",
							"WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"\r\n",
							"//MATCH (ey:EY:Organization{key:'ey'})\r\n",
							"\r\n",
							"MERGE (per:Person{key:line.per_key})\r\n",
							"ON CREATE SET per.name = line.entrepreneurs_name\r\n",
							"ON CREATE SET per.email = line.entrepreneurs_email\r\n",
							"ON CREATE SET per.phone = line.entrepreneurs_mobile_phone\r\n",
							"ON CREATE SET per.createdAt = timeStamp\r\n",
							"SET per.updatedAt = timeStamp\r\n",
							"\r\n",
							"MERGE (psv:PersonSourceVersion:Version{id:'ENT'+ line.per_id, source:'ent'})\r\n",
							"ON CREATE SET psv.createdAt = timeStamp\r\n",
							"SET psv.updatedAt = timeStamp\r\n",
							"SET psv.name = line.entrepreneurs_name\r\n",
							"SET psv.email = line.entrepreneurs_email\r\n",
							"SET psv.phone = line.entrepreneurs_mobile_phone\r\n",
							"\r\n",
							"MERGE (per)-[:HAS_VERSION]->(psv)\r\n",
							"\r\n",
							"Merge (org:Organization{key:line.org_key})\r\n",
							"ON CREATE SET org.name = line.company_name\r\n",
							"ON CREATE SET org.address = line.company_address\r\n",
							"ON CREATE SET org.postalCode = line.company_postal_code\r\n",
							"ON CREATE SET org.phone = line.company_telephone\r\n",
							"ON CREATE SET org.foundedOn = line.company_founded_on\r\n",
							"ON CREATE SET org.homepageURL = line.company_website\r\n",
							"ON CREATE SET org.createdAt = timeStamp\r\n",
							"SET org.updatedAt = timeStamp\r\n",
							"SET org.tradingName = line.company_trading_name\r\n",
							"SET org.isOwned = CASE line.company_owned_flag WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END \r\n",
							"SET org.ownedBy = line.company_owned_name\r\n",
							"SET org.geoReach = line.company_geo_reach\r\n",
							"SET org.duns = line.company_duns_no\r\n",
							"SET org.holding = CASE WHEN trim(COALESCE(line.company_public_private_flag, '')) <> '' THEN  trim(line.company_public_private_flag) ELSE NULL END\r\n",
							"SET org.goPublicYear = line.company_go_public_year\r\n",
							"SET org.tradingSymbol = line.company_trading_symbol\r\n",
							"SET org.connectionToEYType = 'direct'\r\n",
							"\r\n",
							"MERGE (osv:OrgSourceVersion:Version{id:'ENT'+line.org_id, source:'ent'})\r\n",
							"ON CREATE SET osv.createdAt = timeStamp\r\n",
							"SET osv.updatedAt = timeStamp\r\n",
							"SET osv.name = line.company_name\r\n",
							"SET osv.tradingName = line.company_trading_name\r\n",
							"SET osv.address = line.company_address\r\n",
							"SET osv.country = line.company_country\r\n",
							"SET osv.postalCode = line.company_postal_code\r\n",
							"SET osv.region = line.company_region_state\r\n",
							"SET osv.city = line.company_city\r\n",
							"SET osv.phone = line.company_telephone\r\n",
							"SET osv.socialMedia = line.social_media\r\n",
							"SET osv.isOwned = CASE line.company_owned_flag WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END \r\n",
							"SET osv.ownedBy = line.company_owned_name\r\n",
							"SET osv.foundedOn = line.company_founded_on\r\n",
							"SET osv.geoReach = line.company_geo_reach\r\n",
							"SET osv.duns = line.company_duns_no\r\n",
							"SET osv.homepageURL = line.company_website\r\n",
							"SET osv.holding = CASE WHEN trim(COALESCE(line.company_public_private_flag, '')) <> '' THEN  trim(line.company_public_private_flag) ELSE NULL END\r\n",
							"SET osv.goPublicYear = line.company_go_public_year\r\n",
							"SET osv.tradingSymbol = line.company_trading_symbol\r\n",
							"\r\n",
							"MERGE (org)-[:HAS_VERSION]->(osv)\r\n",
							"\r\n",
							"MERGE (per)-[wi:WORKS_IN]->(org)\r\n",
							"ON CREATE SET wi.source = 'ent'\r\n",
							"SET wi.jobTitle = line.entrepreneurs_job_title\r\n",
							"SET wi.phone = line.entrepreneurs_work_phone\r\n",
							"SET wi.isFounder = CASE line.entrepreneurs_company_founder WHEN 'Yes' THEN true WHEN 'No' THEN false ELSE null END\r\n",
							"SET wi.percentOwned = line.entrepreneurs_company_founder\r\n",
							"\r\n",
							"MERGE (e:Event{key:line.event_key})\r\n",
							"ON CREATE SET e.createdAt = timeStamp\r\n",
							"SET e.updatedAt = timeStamp\r\n",
							"SET e.name = line.event_name\r\n",
							"SET e.type = CASE WHEN line.source = 'EOY UX' THEN 'EOY' ELSE line.source END\r\n",
							"SET e.area = line.event_area\r\n",
							"SET e.region = line.event_region\r\n",
							"SET e.year = line.event_year\r\n",
							"SET e.parentSource = 'ent'\r\n",
							"SET e.source = CASE WHEN line.source = 'EOY UX' THEN 'eoy' ELSE TOLOWER(line.source) END\r\n",
							"\r\n",
							"MERGE (per)-[pip:IS_PARTICIPANT]->(e)\r\n",
							"ON CREATE SET pip.createdAt = timeStamp\r\n",
							"SET pip.updatedAt = timeStamp\r\n",
							"SET pip.nominationId = line.event_nomination_id\r\n",
							"SET pip.nominationDate = line.event_nomination_date\r\n",
							"SET pip.source = 'ent'\r\n",
							"\r\n",
							"MERGE (org)-[oip:IS_PARTICIPANT]->(e)\r\n",
							"ON CREATE SET oip.createdAt = timeStamp\r\n",
							"SET oip.updatedAt = timeStamp\r\n",
							"SET oip.nominationId = line.event_nomination_id\r\n",
							"SET oip.nominationDate = line.event_nomination_date\r\n",
							"SET oip.source = 'ent'\r\n",
							"\r\n",
							"// EY Alumni\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_alumini_tier IS NOT NULL AND line.ey_alumini_tier <> '' THEN [1] ELSE [] END | \r\n",
							"    MERGE (per)-[awi:WORKS_IN]->(ey)\r\n",
							"    SET awi.source = 'ent'\r\n",
							"    SET awi.isActive = false\r\n",
							"    SET awi.isPrimary = false \r\n",
							"    SET awi.tier = line.ey_alumini_tier\r\n",
							")\r\n",
							"\r\n",
							"// Other EY data\r\n",
							"FOREACH(ignore IN CASE WHEN line.is_ey IN ['True', 'TRUE', 'true', true] THEN [1] ELSE [] END | \r\n",
							"    MERGE (e)-[ict:IS_CONNECTED_TO]->(ey)\r\n",
							"    SET ict.relationshipManager = line.ey_relationship_mgr\r\n",
							"    SET ict.host = line.ey_host\r\n",
							"    SET ict.lastTargetDate = line.ey_last_target_date\r\n",
							"    SET ict.targetNotes = line.ey_target_notes\r\n",
							"    SET ict.gnFlag = line.ey_growth_navigator_flag\r\n",
							"    SET ict.gnDate = line.ey_growth_navigator_date\r\n",
							"    SET ict.familyBiz = line.ey_family_biz_flag\r\n",
							")\r\n",
							"\r\n",
							"// EY Client\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_client_flag = 'Yes' THEN [1] ELSE [] END |\r\n",
							"    MERGE (org)-[:HAS_CUSTOMER_INFO{source:'ent'}]->(ci:CustomerInfo)\r\n",
							"    ON CREATE SET ci.id = line.ey_client_id\r\n",
							"    ON CREATE SET ci.createdAt = timeStamp\r\n",
							"    SET ci.updatedAt = timeStamp \r\n",
							"    SET ci.channel = line.ey_channel_nm\r\n",
							"    SET ci.marketSegment = line.ey_market_segment\r\n",
							"\r\n",
							"    SET osv.channel = line.ey_channel_nm\r\n",
							"    SET osv.clientId = line.ey_client_id\r\n",
							"\r\n",
							"    SET org.channel = line.ey_channel_nm\r\n",
							"    SET org.clientId = line.ey_client_id\r\n",
							")\r\n",
							"\r\n",
							"// EY IndustrySector (ey_industry transformed - & to and)\r\n",
							"FOREACH(ignore IN CASE WHEN line.ey_industry IS NOT NULL AND line.ey_industry <> '' THEN [1] ELSE [] END | \r\n",
							"    MERGE(sec:Sector{id:TOLOWER(line.ey_industry)})\r\n",
							"    ON CREATE SET sec.name = line.ey_industry\r\n",
							"    ON CREATE SET sec.createdAt = timeStamp\r\n",
							"    SET sec.updatedAt = timeStamp\r\n",
							"    \r\n",
							"    MERGE (org)-[ii:IS_IN]->(sec)\r\n",
							"    ON CREATE SET ii.createdAt = timeStamp\r\n",
							"    SET ii.updatedAt = timeStamp\r\n",
							"    SET ii.source = 'ent')\"\"\"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_load_run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/load_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "2f0c0ce8-b941-432d-9b3c-019a428b2ba6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/neo_connector"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/config/ent_config"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/config/ent_logger_config"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/load_operator/queries/ent_cql"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"#from ce_pipelines import DataLakeAdapter\r\n",
							"#from ce_pipelines import GraphAdapter\r\n",
							"#from ce_pipelines.eoy import config,logger\r\n",
							"\r\n",
							"#from .queries import QuerySelector\r\n",
							"\r\n",
							"\r\n",
							"def execute(entities=['ent'], **kwargs):\r\n",
							"\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    \r\n",
							"    graph = GraphAdapter(config=config, logger=logger)\r\n",
							"    data_lake_conn = DataLakeAdapter(config=config, logger=logger)\r\n",
							"\r\n",
							"    entityquery_dict = {\"ent\":ent_cypher}\r\n",
							"\r\n",
							"    storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    logger.info(f\"Storage Account Name - {storageaccountname} - Started.\")\r\n",
							"    storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    logger.info(f\"Storage Container Name - {storagecontainername} - Started.\")\r\n",
							"    storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storageaccountkey)\r\n",
							"\r\n",
							"    for entity in entities:\r\n",
							"        \r\n",
							"        try:\r\n",
							"            logger.info(f\"Load - {entity} - Started.\")\r\n",
							"            remote_files = data_lake_conn.get_files(env='landing', entity=entity)\r\n",
							"            # landing_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net'+'/'+config[\"storage.landing_dir_name\"]+'/'+(entity)\r\n",
							"            # logger.info(f\"Run: landing_path - {landing_path}.\")\r\n",
							"            # remote_files = mssparkutils.fs.ls(landing_path)\r\n",
							"            print(remote_files)\r\n",
							"            for remote_file in remote_files:\r\n",
							"                logger.info(f\"File Name: {remote_file} Load \")\r\n",
							"                # remote_file_path = f\"config[\"storage.landing_dir_name\"]/\"\r\n",
							"                sas_url = data_lake_conn.generate_sas_url(remote_file)\r\n",
							"                logger.info(f\"File sas_url: {sas_url} \")\r\n",
							"                if sas_url:\r\n",
							"                    logger.info(f\"File sas_url: {sas_url} \")\r\n",
							"                    # query = QuerySelector(config=config, logger=logger).build_query(entity, sas_url)\r\n",
							"                    query_template = entityquery_dict[entity]\r\n",
							"                    query = query_template.replace('<<SAS_URL>>', sas_url)\r\n",
							"                    logger.info(f\"For {entity} query - {query}.\")\r\n",
							"                    if query:    \r\n",
							"                        graph.execute_query(query)\r\n",
							"                    \r\n",
							"            logger.info(f\"Load - {entity} - Completed.\")\r\n",
							"        except Exception as e:\r\n",
							"            logger.error(str(e))\r\n",
							"            logger.info(f\"Load - {entity} - Failed.\")\r\n",
							"\r\n",
							"    graph.close()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute()"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_logger_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "88164edf-328d-4345-b30f-53d0440a75f9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import logging\r\n",
							"\r\n",
							"# logger config\r\n",
							"# log levels: debug, info, warning, error, critical\r\n",
							"logFormatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s',datefmt='%Y-%m-%d %H:%M:%S')\r\n",
							"logger = logging.getLogger('ENT')\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"# stream logger\r\n",
							"sh = logging.StreamHandler() # stream handler\r\n",
							"sh.setFormatter(logFormatter)\r\n",
							"sh.setLevel(logging.INFO)\r\n",
							"logger.addHandler(sh)"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_query_selector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/load_operator/queries"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e794a2f9-5a34-4627-a980-ec4951734cc9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\r\n",
							"\r\n",
							"class QuerySelector:\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"\r\n",
							"    def build_query(self, entity, sas_url):\r\n",
							"        query = None\r\n",
							"        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), f\"{entity}.cql\")) as file:\r\n",
							"            query_template = file.read()\r\n",
							"            query = query_template.replace('<<SAS_URL>>', sas_url)\r\n",
							"        return query  \r\n",
							"\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/transformation_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "096f5103-d4bc-4115-96f2-43d53902acec"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/transformation_operator/ent_transformation"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/config/ent_config"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/config/ent_logger_config"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"\r\n",
							"#from ce_pipelines import DataLakeAdapter\r\n",
							"#from ce_pipelines.eoy import config,logger\r\n",
							"\r\n",
							"\r\n",
							"#from .transformations import Transformations\r\n",
							"\r\n",
							"def execute(entities=['ent'],**kwargs):\r\n",
							"        \r\n",
							"    storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storageaccountname)\r\n",
							"    storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storagecontainername)\r\n",
							"    # storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    # print(storageaccountkey)\r\n",
							"\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    for entity in entities:\r\n",
							"        print(entity)\r\n",
							"    \r\n",
							"        logger.info(f\"Transformation - {entity} - Started.\")\r\n",
							"        file_name = f'{entity}.csv'\r\n",
							"        # local_temp_file_paths = []\r\n",
							"\r\n",
							"        file_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"        logger.info(f\"Source File path - {file_path} .\")\r\n",
							"        # data_lake_conn = DataLakeAdapter(config=config, logger=logger, env='landing', entity=entity)\r\n",
							"\r\n",
							"\r\n",
							"        #transformed_file_path = TransformationController(config=config, logger=logger).transform(file_path, entity) \r\n",
							"        transformed_file_path = Transformations(config=config, logger=logger).transform(file_path, entity)\r\n",
							"        logger.info(f\"Transformation; File Location: {transformed_file_path}\")\r\n",
							"        if transformed_file_path:\r\n",
							"            #local_temp_file_paths.append(transformed_file_path)\r\n",
							"            logger.info(f\"Transformation completed; File Location: {transformed_file_path}\")\r\n",
							"            print(\"myfile\",transformed_file_path)\r\n",
							"            # data_lake_conn.split_upload_file_from_local(transformed_file_path, entity, env='landing')\r\n",
							"            DataLakeAdapter(config=config, logger=logger, env='landing', entity=entity).split_upload_file_from_local(transformed_file_path, entity, env='landing')\r\n",
							"\r\n",
							"        \r\n",
							"        # # Delete temporary files\r\n",
							"\r\n",
							"        # #for local_temp_file_path in local_temp_file_paths:\r\n",
							"        #     #if os.path.exists(local_temp_file_path):\r\n",
							"        #        # os.remove(local_temp_file_path)\r\n",
							"        #         #logger.info(f\"File deleted - {local_temp_file_path}\")\r\n",
							"        #     #else:\r\n",
							"        #         #logger.info(f\"File doesn't exist - {local_temp_file_path}\")\r\n",
							"\r\n",
							"        # #logger.info(f\"Transformation - {entity} - Completed.\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ent_transformation')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/ent/transformation_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf1f5706-19b6-44cc-bf49-8b6039cc8b9c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/transformation_controller"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/ent/config/ent_config"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"import json\r\n",
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import csv\r\n",
							"import hashlib\r\n",
							"\r\n",
							"class Transformations(TransformationController):\r\n",
							"\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        super().__init__(config, logger)\r\n",
							"        self.local_cache = {\r\n",
							"            \"org_key_lookup.csv\": None,\r\n",
							"            \"emp_key_lookup.csv\": None\r\n",
							"        }\r\n",
							"\r\n",
							"    def generate_org_id(self, org_name):\r\n",
							"        text = org_name.lower().strip()\r\n",
							"        return hashlib.md5(text.encode()).hexdigest()\r\n",
							"\r\n",
							"    def generate_per_id(self, val):\r\n",
							"        ent_name = val[0]\r\n",
							"        org_name = val[1]\r\n",
							"        text = ent_name.lower().strip() + ' ' + org_name.lower().strip()\r\n",
							"        return hashlib.md5(text.encode()).hexdigest()\r\n",
							"\r\n",
							"    def generate_event_key(self, val):\r\n",
							"        # \"source\",\"event_name\",\"event_area\",\"event_region\",\"event_year\"\r\n",
							"        text = ' '.join([v.lower().strip() for v in val if v])\r\n",
							"        return hashlib.md5(text.encode()).hexdigest()\r\n",
							"\r\n",
							"    def list_social_media_ids(self, val):\r\n",
							"        company_social_media = val[0]\r\n",
							"        company_username = val[1]\r\n",
							"        sm_names = company_social_media.split(',')\r\n",
							"        sm_ids = company_username.split(',')\r\n",
							"\r\n",
							"        sm_lst = [] \r\n",
							"        if len(sm_ids) == len(sm_names):\r\n",
							"            for name, id in zip(sm_names,sm_ids):\r\n",
							"                sm_lst.append(f'{name}::{id}')\r\n",
							"        return ';'.join(sm_lst)\r\n",
							"        \r\n",
							"    def prep_ey_industry(self, ey_industry):\r\n",
							"        return ey_industry.replace('&', 'and').strip()\r\n",
							"\r\n",
							"    def generate_sector_id(self,text):\r\n",
							"        # Convert text to lower\r\n",
							"        text = text.lower()\r\n",
							"\r\n",
							"        # Replace 'and' and '&' with \"\"\r\n",
							"        text = text.replace(\"and\", \"\")\r\n",
							"        text = text.replace(\"&\", \"\")\r\n",
							"\r\n",
							"        # Consider only alphabets and space\r\n",
							"        text = re.sub(\"[^a-z\\s]\", \"\", text)\r\n",
							"\r\n",
							"        # Replace 'real estate' with re\r\n",
							"        text = text.replace(\"real estate\", \"re\")\r\n",
							"\r\n",
							"        # Split text into tokens\r\n",
							"        text_tokens = text.split(\" \")\r\n",
							"\r\n",
							"        # Join without space\r\n",
							"        text = \"\".join(x for x in text_tokens if len(x) != 0)\r\n",
							"\r\n",
							"        return text\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ea947b78-192b-4dc6-bbfc-34bc77d901c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"com_config = config"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import config as base_config\r\n",
							"\r\n",
							"config = {\r\n",
							"    \"source_flag\": \"EOY\",\r\n",
							"    \"storage.staging_dir_name\":\"data/staging/eoy\",\r\n",
							"    \"storage.chunksize\":200,\r\n",
							"    \"storage.landing_dir_name\":\"data/landing/eoy\",\r\n",
							"    \"data.chunksize\":10000,\r\n",
							"\r\n",
							"    \"transformations\": {\r\n",
							"        \"eoy\":{\r\n",
							"            \"invalid_columns\":[], \r\n",
							"            \"mappings\":[\r\n",
							"                {\r\n",
							"                    \"function\": \"get_org_key\",\r\n",
							"                    \"src_column_names\": \"company_id\",\r\n",
							"                    \"dst_column_names\": \"org_key\"\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"function\": \"get_emp_key\",\r\n",
							"                    \"src_column_names\": \"entrepreneurs_id\",\r\n",
							"                    \"dst_column_names\": \"per_key\"\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        }\r\n",
							"    }\r\n",
							"}\r\n",
							"\r\n",
							"#config.update(base_config)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"config.update(com_config)\r\n",
							"print(config)"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_cql')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/load_operator/queries"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "51990b18-da41-4c77-a7b0-91447ee9e2aa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"eoy_cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\"  AS line  \r\n",
							"\r\n",
							"MERGE (org:Organization{key:line.org_key})\r\n",
							"ON CREATE SET org.name = line.entrepreneurs_company\r\n",
							"SET org.connectionToEYType = 'direct'\r\n",
							"\r\n",
							"// Backward Compatibility\r\n",
							"MERGE (osv:OrgSourceVersion:Version{id:line.org_key})\r\n",
							"ON CREATE SET osv.name = line.entrepreneurs_company\r\n",
							"ON CREATE SET osv.source = CASE WHEN line.org_key STARTS WITH 'CB' THEN 'crunchbase' ELSE 'eoy' END  \r\n",
							"ON CREATE SET osv.priority = 1\r\n",
							"\r\n",
							"MERGE (org)-[:HAS_VERSION]->(osv) \r\n",
							"\r\n",
							"MERGE (npn:Person{key:line.per_key})\r\n",
							"ON CREATE SET npn.name = line.entrepreneurs_name\r\n",
							"\r\n",
							"// Backward Compatibility\r\n",
							"MERGE (psv:PersonSourceVersion:Version{id:line.per_key})\r\n",
							"ON CREATE SET psv.name = line.entrepreneurs_name\r\n",
							"ON CREATE SET psv.source = CASE WHEN line.per_key STARTS WITH 'CB' THEN 'crunchbase' ELSE 'eoy' END  \r\n",
							"ON CREATE SET psv.priority = 1\r\n",
							"\r\n",
							"\r\n",
							"MERGE (npn)-[:HAS_VERSION]->(psv)\r\n",
							"\r\n",
							"MERGE (event:Event{source:\"eoy\", name:line.event_name, year:line.event_year})\r\n",
							"ON CREATE SET event.createdAt = datetime()\r\n",
							"SET event.updatedAt = datetime()\r\n",
							"SET event.type = 'EOY'\r\n",
							"SET event.parentSource = 'eoy'\r\n",
							"SET event.source = 'eoy'\r\n",
							"\r\n",
							"MERGE (npn)-[ipv:IS_PARTICIPANT{orgKey:line.org_key,orgName:line.entrepreneurs_company,nominationId:line.nomination_id}]->(event)\r\n",
							"MERGE (org)-[ip:IS_PARTICIPANT{perKey:line.per_key,perName:line.entrepreneurs_name,nominationId:line.nomination_id}]->(event)\r\n",
							"SET ip.isClient = CASE line.is_client WHEN 'Yes' THEN true ELSE false END\r\n",
							"SET ip.isTarget = CASE line.is_target WHEN 'Yes' THEN true ELSE false END\r\n",
							"SET ip.achievement = line.status_achieved\r\n",
							"\r\n",
							"SET ipv.isClient = CASE line.is_client WHEN 'Yes' THEN true ELSE false END\r\n",
							"SET ipv.isTarget = CASE line.is_target WHEN 'Yes' THEN true ELSE false END\r\n",
							"SET ipv.achievement = line.status_achieved\r\n",
							"\r\n",
							"MERGE (npn)-[wi:WORKS_IN]->(org)\r\n",
							"\r\n",
							"ON CREATE SET wi.source = 'eoy'\r\n",
							"\r\n",
							"// Backward Compatibility\r\n",
							"MERGE(esv:EventSourceVersion:Version{source:\"eoy\", name:line.event_name, year:line.event_year, priority:1})\r\n",
							"MERGE(esv)<-[:HAS_VERSION]-(event)\r\n",
							"WITH DISTINCT line, esv\r\n",
							"MATCH(cbpsv:PersonSourceVersion:Version{id:line.per_key})\r\n",
							"WITH DISTINCT line, esv, cbpsv\r\n",
							"MERGE(cbpsv)-[sp:IS_PARTICIPANT]->(esv)\r\n",
							"ON CREATE SET sp.achievement = line.status_achieved\"\"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_logger_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "49db7de9-1b5d-434b-93a8-95e58f3ff73d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import logging\r\n",
							"\r\n",
							"# logger config\r\n",
							"# log levels: debug, info, warning, error, critical\r\n",
							"logFormatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s',datefmt='%Y-%m-%d %H:%M:%S')\r\n",
							"logger = logging.getLogger('EOY')\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"# stream logger\r\n",
							"sh = logging.StreamHandler() # stream handler\r\n",
							"sh.setFormatter(logFormatter)\r\n",
							"sh.setLevel(logging.INFO)\r\n",
							"logger.addHandler(sh)"
						],
						"outputs": [],
						"execution_count": 56
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/load_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "20ccdfb6-5695-44b2-813c-c7b12463df27"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/neo_connector"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/logger_config"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %run ce_pipelines/eoy/load_operator/queries/query_selector"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/load_operator/queries/eoy_cql"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"def execute(entities=['eoy'], **kwargs):\r\n",
							"\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    \r\n",
							"    graph = GraphAdapter(config=config, logger=logger)\r\n",
							"    data_lake_conn = DataLakeAdapter(config=config, logger=logger)\r\n",
							"\r\n",
							"    entityquery_dict = {\"eoy\":eoy_cypher}\r\n",
							"\r\n",
							"    storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    logger.info(f\"Storage Account Name - {storageaccountname} - Started.\")\r\n",
							"    storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    logger.info(f\"Storage Container Name - {storagecontainername} - Started.\")\r\n",
							"    storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storageaccountkey)\r\n",
							"\r\n",
							"    for entity in entities:\r\n",
							"        \r\n",
							"        try:\r\n",
							"            logger.info(f\"Load - {entity} - Started.\")\r\n",
							"            remote_files = data_lake_conn.get_files(env='landing', entity=entity)\r\n",
							"            logger.info(f\"remote_files: {remote_files} Load \")\r\n",
							"            for remote_file in remote_files:\r\n",
							"                logger.info(f\"File Name: {remote_file} Load \")\r\n",
							"                sas_url = data_lake_conn.generate_sas_url(remote_file)\r\n",
							"                logger.info(f\"File sas_url: {sas_url} \")\r\n",
							"                if sas_url:\r\n",
							"                    logger.info(f\"File sas_url: {sas_url} \")\r\n",
							"                    query_template = entityquery_dict[entity]\r\n",
							"                    query = query_template.replace('<<SAS_URL>>', sas_url)\r\n",
							"                    logger.info(f\"For {entity} query - {query}.\")\r\n",
							"                    if query:    \r\n",
							"                        graph.execute_query(query)\r\n",
							"                    \r\n",
							"            logger.info(f\"Load - {entity} - Completed.\")\r\n",
							"        except Exception as e:\r\n",
							"            logger.error(str(e))\r\n",
							"            logger.info(f\"Load - {entity} - Failed.\")\r\n",
							"\r\n",
							"    graph.close()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute()"
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_trans_run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/transformation_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "0c88ecb8-f28b-45c1-bb82-0e19e59d3af3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/transformation_operator/eoy_transformations"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(config)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_logger_config"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"\r\n",
							"#from ce_pipelines import DataLakeAdapter\r\n",
							"#from ce_pipelines.eoy import config,logger\r\n",
							"\r\n",
							"\r\n",
							"#from .transformations import Transformations\r\n",
							"\r\n",
							"def execute(entities=['eoy'],**kwargs):\r\n",
							"        \r\n",
							"    storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storageaccountname)\r\n",
							"    storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    print(storagecontainername)\r\n",
							"    # storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"    # print(storageaccountkey)\r\n",
							"\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    for entity in entities:\r\n",
							"        print(entity)\r\n",
							"    \r\n",
							"        logger.info(f\"Transformation - {entity} - Started.\")\r\n",
							"        file_name = f'{entity}.csv'\r\n",
							"        # local_temp_file_paths = []\r\n",
							"\r\n",
							"        file_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"        logger.info(f\"Source File path - {file_path} .\")\r\n",
							"        # data_lake_conn = DataLakeAdapter(config=config, logger=logger, env='landing', entity=entity)\r\n",
							"\r\n",
							"\r\n",
							"        #transformed_file_path = TransformationController(config=config, logger=logger).transform(file_path, entity) \r\n",
							"        transformed_file_path = Transformations(config=config, logger=logger).transform(file_path, entity)\r\n",
							"        logger.info(f\"Transformation; File Location: {transformed_file_path}\")\r\n",
							"        if transformed_file_path:\r\n",
							"            #local_temp_file_paths.append(transformed_file_path)\r\n",
							"            logger.info(f\"Transformation completed; File Location: {transformed_file_path}\")\r\n",
							"            print(\"myfile\",transformed_file_path)\r\n",
							"            # data_lake_conn.split_upload_file_from_local(transformed_file_path, entity, env='landing')\r\n",
							"            DataLakeAdapter(config=config, logger=logger, env='landing', entity=entity).split_upload_file_from_local(transformed_file_path, entity, env='landing')\r\n",
							"\r\n",
							"        \r\n",
							"        # # Delete temporary files\r\n",
							"\r\n",
							"        # #for local_temp_file_path in local_temp_file_paths:\r\n",
							"        #     #if os.path.exists(local_temp_file_path):\r\n",
							"        #        # os.remove(local_temp_file_path)\r\n",
							"        #         #logger.info(f\"File deleted - {local_temp_file_path}\")\r\n",
							"        #     #else:\r\n",
							"        #         #logger.info(f\"File doesn't exist - {local_temp_file_path}\")\r\n",
							"\r\n",
							"        # #logger.info(f\"Transformation - {entity} - Completed.\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute()"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eoy_transformations')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/transformation_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "94e01b0f-7476-4c82-b79a-4f908e2906df"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/transformation_controller"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"import json\r\n",
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"#from ce_pipelines import TransformationController\r\n",
							"\r\n",
							"class Transformations(TransformationController):\r\n",
							"\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        super().__init__(config, logger)\r\n",
							"        self.local_cache = {\r\n",
							"            \"org_key_lookup.csv\": None,\r\n",
							"            \"emp_key_lookup.csv\": None\r\n",
							"        }    \r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ermsCypher')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5b358bd8-426a-4fe4-afc3-8bd63a5c34c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"cypher = \"\"\"LOAD CSV WITH HEADERS FROM \"https://bmdatalakestorage2023.blob.core.windows.net/bmfilesystem2023/data/landing/erms/organizations_inc/part-00000.csv?sp=r&st=2023-02-01T11:50:20Z&se=2023-02-02T19:50:20Z&spr=https&sv=2021-06-08&sr=b&sig=Q764ixiccprFrjAf5T5UcptEt4aXXPL%2BE5vsNut9jkU%3D\" AS line\r\n",
							"    WITH line, apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"    MERGE (norg:Organization{key:line.org_key})\r\n",
							"    ON CREATE SET norg.role = line.role\"\"\""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/etl_status_recorder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8442684f-0cb0-412b-a779-e7003798548c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run neo_connector"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#from .neo_connector import GraphAdapter\r\n",
							"\r\n",
							"class ETLStatusRecorder:\r\n",
							"    \r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        \r\n",
							"        self._graph = GraphAdapter(self._config, self._logger)\r\n",
							"\r\n",
							"    def update_status(self, status, phase=None):\r\n",
							"        query = None        \r\n",
							"        if not phase:\r\n",
							"            query = '''\r\n",
							"                WITH \r\n",
							"\t                $etl_id AS etlId,\r\n",
							"\t                $status AS status,\r\n",
							"\t                apoc.date.format(datetime().epochMillis,\"ms\", \"yyyy-MM-dd HH:mm:ss\") AS timeStamp\r\n",
							"                MERGE (etlC:ETLCore)\r\n",
							"                MERGE (etlC)-[:HAS_SOURCE]->(etl:ETL{id:etlId})\r\n",
							"                SET etl.lastExecutionAt = timeStamp\r\n",
							"                SET etl.lastSuccessfulExecutionAt = CASE status WHEN 'success' THEN timeStamp ELSE etl.lastSuccessfulExecutionAt END\r\n",
							"            '''\r\n",
							"            self._graph.execute_update_query(query, etl_id=self._config['source_flag'], status=status)\r\n",
							"\r\n",
							"    def _format_date(self, date_time_str):\r\n",
							"        return date_time_str\r\n",
							"\r\n",
							"    def get_last_successful_execution_at(self):\r\n",
							"        query = '''\r\n",
							"            WITH $etl_id AS etlId\r\n",
							"            MATCH (:ETLCore)-[:HAS_SOURCE]->(etl:ETL{id:etlId})\r\n",
							"            RETURN etl.lastSuccessfulExecutionAt AS last_successful_execution_at\r\n",
							"        '''\r\n",
							"        res = self._graph.execute_read_query(query, etl_id=self._config['source_flag'])\r\n",
							"        if res:\r\n",
							"            return self._format_date(res[0]['last_successful_execution_at'])\r\n",
							"        else:\r\n",
							"            return None\r\n",
							"\r\n",
							"    def __del__(self):\r\n",
							"\r\n",
							"        if self._graph:\r\n",
							"            self._graph.close()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 33
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/extractor')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/crunchbase/extraction_operator/organizations"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c24da44c-bf53-4708-854a-a0444f70f676"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\r\n",
							"import pandas as pd \r\n",
							"from datetime import datetime, timedelta \r\n",
							"import io\r\n",
							"\r\n",
							"class OrganizationsDataExtractor:\r\n",
							"\r\n",
							"    entity = \"organizations\"\r\n",
							"    \r\n",
							"    def __init__(self, staging, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._staging = staging\r\n",
							"        self._data = {\r\n",
							"            \"organizations_inc\": pd.DataFrame(),\r\n",
							"            \"organizations_geo_inc\": pd.DataFrame()\r\n",
							"        }\r\n",
							"    \r\n",
							"    def _upload(self):\r\n",
							"        for output_entity, df in self._data.items():\r\n",
							"            if not df.empty:\r\n",
							"                data = io.StringIO()\r\n",
							"                df.to_csv(data, index=False)\r\n",
							"                self._staging.upload_file(f'{output_entity}.csv', data, 'staging')\r\n",
							"                data.close()\r\n",
							"\r\n",
							"    def extract(self, dir_path:str):\r\n",
							"        \r\n",
							"        from_date = self._config.get('start_date', (datetime.now() - timedelta(days=1)).strftime(self._config['datetime_format']))\r\n",
							"        to_date = self._config.get('end_date', None)\r\n",
							"\r\n",
							"        file_path = os.path.join(dir_path, f\"{self.entity}.csv\")\r\n",
							"        \r\n",
							"        filtered_chunks = []\r\n",
							"        data_chunks = pd.read_csv(file_path, chunksize=10000)\r\n",
							"        for data_chunk in data_chunks:\r\n",
							"            data_chunk['updated_at'] = pd.to_datetime(data_chunk['updated_at'], format=self._config['datetime_format'])\r\n",
							"            if to_date:\r\n",
							"                filtered_chunks.append(data_chunk[(data_chunk['updated_at'] >= from_date) & (data_chunk['updated_at'] < to_date)])\r\n",
							"            else:\r\n",
							"                filtered_chunks.append(data_chunk[(data_chunk['updated_at'] >= from_date)])\r\n",
							"\r\n",
							"        ids = set()\r\n",
							"        if filtered_chunks:\r\n",
							"            self._data['organizations_inc'] = pd.concat(filtered_chunks, ignore_index=True)\r\n",
							"            self._extract_geo()\r\n",
							"            self._upload()\r\n",
							"            ids = set(self._data['organizations_inc']['uuid'])\r\n",
							"\r\n",
							"        self._logger.info(f'{len(ids)} records extracted and staged from entity - {self.entity}')\r\n",
							"        return ids\r\n",
							"    \r\n",
							"    def _extract_geo(self):\r\n",
							"        self._data['organizations_geo_inc'] = self._data['organizations_inc'][['country_code','region','city', 'uuid']].dropna(how='any', subset=['country_code']).groupby(['country_code','region','city'])['uuid'].apply(lambda x: ','.join(list(x))).reset_index(name='uuid')\r\n",
							"        self._data['organizations_inc'].drop(columns=['country_code','region','city'], inplace=True)        "
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/githubtestnotebbok')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bf222c44-1b5b-4fbe-b63d-517e7e8894ed"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(\"hi\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "edw"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "351ba4b4-7baa-48bc-a519-0bcbcc71501c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run common/neo_connector"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"import tempfile\r\n",
							"import os\r\n",
							"\r\n",
							"from ce_pipelines import DataLakeAdapter\r\n",
							"from ce_pipelines import GraphAdapter\r\n",
							"from ce_pipelines.edw import config,logger\r\n",
							"from ce_pipelines import Archive\r\n",
							"\r\n",
							"from .queries import QuerySelector\r\n",
							"\r\n",
							"\r\n",
							"def execute(entities=['customers_inc',\r\n",
							"    'vendors_inc',\r\n",
							"    'employee_inc',\r\n",
							"    'employee_account_assignment_inc',\r\n",
							"    'employee_job_inc',\r\n",
							"    'employee_phone_inc',\r\n",
							"    'employee_competency_hierarchy_inc',\r\n",
							"    'employee_competency_assessment_inc',\r\n",
							"    'employee_badge_request_inc',\r\n",
							"    'employee_badge_request_attributes_inc'], **kwargs):\r\n",
							"    '''\r\n",
							"    entities=['customers_inc',\r\n",
							"    'vendors_inc',\r\n",
							"    'employee_inc',\r\n",
							"    'employee_account_assignment_inc',\r\n",
							"    'employee_job_inc',\r\n",
							"    'employee_phone_inc',\r\n",
							"    'employee_competency_hierarchy_inc',\r\n",
							"    'employee_competency_assessment_inc',\r\n",
							"    'employee_badge_request_inc',\r\n",
							"    'employee_badge_request_attributes_inc'\r\n",
							"    '''\r\n",
							"    # Update Default Config - Optional\r\n",
							"    config.update(kwargs)\r\n",
							"    \r\n",
							"    graph = GraphAdapter(config=config, logger=logger)\r\n",
							"    data_lake_conn = DataLakeAdapter(config=config, logger=logger)\r\n",
							"\r\n",
							"    for entity in entities:\r\n",
							"        \r\n",
							"        try:\r\n",
							"            logger.info(f\"Load - {entity} - Started.\")\r\n",
							"            remote_files = data_lake_conn.get_files(env='landing', entity=entity) \r\n",
							"            for remote_file in remote_files:\r\n",
							"                sas_url = data_lake_conn.generate_sas_url(remote_file)\r\n",
							"                if sas_url:\r\n",
							"                    query = QuerySelector(config=config, logger=logger).build_query(entity, sas_url)\r\n",
							"                    if query:    \r\n",
							"                        graph.execute_query(query)\r\n",
							"                    \r\n",
							"            logger.info(f\"Load - {entity} - Completed.\")\r\n",
							"        except Exception as e:\r\n",
							"            logger.error(str(e))\r\n",
							"            logger.info(f\"Load - {entity} - Failed.\")\r\n",
							"\r\n",
							"    graph.close()\r\n",
							"\r\n",
							"    # Running Archive module\r\n",
							"    archive_obj = Archive(config, entity=config.get(\"source_flag\"))\r\n",
							"    archive_obj.move_folder()"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/logger_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/crunchbase/crunchbase_config"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b951ff1f-ba8f-4a51-ae54-4da0485923ed"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import logging\r\n",
							"\r\n",
							"# logger config\r\n",
							"# log levels: debug, info, warning, error, critical\r\n",
							"logFormatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s',datefmt='%Y-%m-%d %H:%M:%S')\r\n",
							"logger = logging.getLogger('Crunchbase Operator')\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"# stream logger\r\n",
							"sh = logging.StreamHandler() # stream handler\r\n",
							"sh.setFormatter(logFormatter)\r\n",
							"sh.setLevel(logging.INFO)\r\n",
							"logger.addHandler(sh)"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/neo_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e30944c9-7a1d-4f41-b2c6-8377854f71e9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install neo4j"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from neo4j import GraphDatabase\r\n",
							"import os\r\n",
							"import time\r\n",
							"\r\n",
							"class GraphAdapter:\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._connector = GraphDatabase.driver(self._config['neo4j.bolt_url'], auth=(self._config['neo4j.user_name'], self._config['neo4j.password']),encrypted=False)\r\n",
							"\r\n",
							"    def execute_query(self, query):\r\n",
							"        connector = None\r\n",
							"        try:\r\n",
							"            self._logger.info(f\"Query execution initiated.\")\r\n",
							"            start = time.time()\r\n",
							"            with self._connector.session(database=self._config['neo4j.database']) as session:\r\n",
							"                res = session.write_transaction(self._execute_txn, query)\r\n",
							"            end = time.time()\r\n",
							"            self._logger.info(f\"Query execution completed. Execution Time - {end - start} seconds.\")\r\n",
							"            return res\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(f\"Query Execution Failed - {e}\")\r\n",
							"\r\n",
							"        \r\n",
							"    def _execute_txn(self, tx, query):\r\n",
							"        res = tx.run(query)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"        return values\r\n",
							"\r\n",
							"\r\n",
							"    def execute_update_query(self, query, **kwargs):\r\n",
							"        connector = None\r\n",
							"        try:\r\n",
							"            with self._connector.session(database=self._config['neo4j.database']) as session:\r\n",
							"                res = session.write_transaction(self._execute_arg_txn, query, kwargs)\r\n",
							"            return res\r\n",
							"        except Exception as e:\r\n",
							"            raise e\r\n",
							"\r\n",
							"    def execute_read_query(self, query, **kwargs):\r\n",
							"        connector = None\r\n",
							"        try:\r\n",
							"            with self._connector.session(database=self._config['neo4j.database']) as session:\r\n",
							"                res = session.read_transaction(self._execute_arg_txn, query, kwargs)\r\n",
							"            return res\r\n",
							"        except Exception as e:\r\n",
							"            raise e\r\n",
							"        \r\n",
							"    def _execute_arg_txn(self, tx, query, data):\r\n",
							"        res = tx.run(query, data)\r\n",
							"        values = []\r\n",
							"        for record in res:\r\n",
							"            values.append(dict(record))\r\n",
							"        return values\r\n",
							"    \r\n",
							"        \r\n",
							"    def close(self):\r\n",
							"        try: \r\n",
							"            if self._connector:\r\n",
							"                self._connector.close()\r\n",
							"        except Exception as e:\r\n",
							"            self._logger.error(f\"Exception while closing the connection - {e}\")\r\n",
							"        \r\n",
							""
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/query_selector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/eoy/load_operator/queries"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bf7d91cf-0df3-4616-ac51-25c4545402c8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import os\r\n",
							"\r\n",
							"class QuerySelector:\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"\r\n",
							"    def build_query(self, entity, sas_url):\r\n",
							"        query = None\r\n",
							"        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), f\"{entity}.cql\")) as file:\r\n",
							"            query_template = file.read()\r\n",
							"            query = query_template.replace('<<SAS_URL>>', sas_url)\r\n",
							"        return query  \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/run')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "run.py",
				"folder": {
					"name": "ce_pipelines/edw/extraction_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d6509f75-7722-4c22-86f2-a90fe6b56527"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(\"hello\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test = DataLakeAdapterr(\"xyz\")"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/run_123')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "bb33e157-8fa9-4508-b287-18e5ab2dadb9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run Transpy"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"res = transform(config,2,3)\r\n",
							"print(res)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"r"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sampleCypher')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "801e1a42-7107-4271-b0ab-8f97a79805ba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"cypher2 = (\"\"\"LOAD CSV WITH HEADERS FROM \"<<SAS_URL>>\"  AS line \r\n",
							"WITH line\r\n",
							"MERGE (per:person{pname:line.name})\r\n",
							"ON CREATE SET per.pdept = line.dept\"\"\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher3 = \"CREATE (a:Person {name: 'Akhilesh Kumar'})\""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher4 = \"CREATE (a:Person {name: 'Satej Zambre'})\""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cypher5 = \"CREATE (a:Person {name: 'Bhairaram Mehrotra'})\""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sampleCypher5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c0e5f418-79fd-43d8-98bf-2714729cb1ad"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"cypher5 = \"CREATE (a:Person {name: 'Mike Husseyi'})\""
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scratchpad')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c1094f82-5081-4c7b-9b39-d27da9f950ed"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"a = mssparkutils.credentials.getSecret('bmehubkeyvault2023','test-kv-sc-adls','ls_bmehubkv2023')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/logger_config"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# get list of files from Azure Data lake storage\r\n",
							"\r\n",
							"\r\n",
							"storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"logger.info(f\"Storage Account Name - {storageaccountname} - Started.\")\r\n",
							"storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"logger.info(f\"Storage Container Name - {storageaccountname} - Started.\")\r\n",
							"storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"entity = 'eoy'\r\n",
							"\r\n",
							"# storagecontainername = 'test'\r\n",
							"# storageaccountname = 'bmdatalakestorage2023'\r\n",
							"landing_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net'+'/'+config[\"storage.landing_dir_name\"]+'/'+(entity)\r\n",
							"logger.info(f\"Storage landing_path - {landing_path}\")\r\n",
							"files_obj_list = mssparkutils.fs.ls(landing_path)\r\n",
							"# files_obj_list = mssparkutils.fs.ls('abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/landing/eoy/eoy')\r\n",
							"print(files_obj_list)\r\n",
							"print(type(files_obj_list))\r\n",
							"for file_obj in files_obj_list:\r\n",
							"    print(file_obj)\r\n",
							"    print(type(file_obj))\r\n",
							"    # finfo = mssparkutils.handlers.fsHandler.FileInfo(file_obj)\r\n",
							"    print(file_obj.name, ' - ', file_obj.isFile)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"help(mssparkutils.handlers.fsHandler.FileInfo)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = pandas.DataFrame({'Name':['A', 'B', 'C', 'D'], 'ID':[20, 21, 19, 18]})\r\n",
							"print(data)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data.to_csv('abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/dummy/data_test.csv')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install neo4j"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import ssl\r\n",
							"# def create_neo4j_driver():\r\n",
							"# auth = basic_auth(NEO4J_USER, NEO4J_PASSWORD)\r\n",
							"ssl_context = ssl.create_default_context()\r\n",
							"ssl_context.load_verify_locations(\"https://bmdatalakestorage2023.blob.core.windows.net/ca-certificates/EY_Enterprise_Root_CA.crt?sp=r&st=2023-03-13T14:47:16Z&se=2023-03-13T22:47:16Z&spr=https&sv=2021-12-02&sr=b&sig=z3tveHlic%2F8ljAPENn37DhnOL03l87Khh%2Bg5o8JxkJo%3D\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"def execute_cypher():\r\n",
							"    mssparkutils.fs.mount(\r\n",
							"    \"abfss://ca-certificates@bmdatalakestorage2023.dfs.core.windows.net\",\r\n",
							"    \"/ca-certificates\",\r\n",
							"    {\"linkedService\":\"AzureDataLakeStorage1\"}\r\n",
							"    )\r\n",
							"\r\n",
							"    jobId = mssparkutils.env.getJobId()\r\n",
							"    print(jobId)\r\n",
							"    c_files = mssparkutils.fs.ls(\"abfss://ca-certificates@bmdatalakestorage2023.dfs.core.windows.net\")\r\n",
							"    cert_files = []\r\n",
							"    for file in c_files:\r\n",
							"        if (file.name.endswith(\".crt\")):\r\n",
							"            cert_files.append(\"synfs:/\"+jobId+\"/ca-certificates/\"+file.name)\r\n",
							"\r\n",
							"    for x in range(len(cert_files)):\r\n",
							"        print(cert_files[x])\r\n",
							"    \r\n",
							"    for cert in cert_files:\r\n",
							"        print(cert)\r\n",
							"        mssparkutils.fs.head(cert)\r\n",
							"        ssl_context.load_verify_locations(cert)\r\n",
							"\r\n",
							"    mssparkutils.fs.unmount(\"/ca-certificates\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.head(\"synfs:/200/ca-certificates/EY_Enterprise_Interim_CA.crt\")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"execute_cypher()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"abfss://ca-certificates@bmdatalakestorage2023.dfs.core.windows.net\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/secondCntrlrNB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dd4ce0d7-8a1e-48a3-8605-627c3c9989b1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"import sys\r\n",
							"current_module = sys.modules[__name__]"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def mul_x(val):\r\n",
							"    x=val[0]\r\n",
							"    y=val[1]\r\n",
							"    print(\"Transpy\")\r\n",
							"    print(\"NB: \",config[\"source_flag\"])\r\n",
							"    z = x*y\r\n",
							"    return z\r\n",
							"\r\n",
							"def transform(config, a, b):\r\n",
							"    if config[\"source_flag\"] == \"EOY\":\r\n",
							"        print(\"secondCntlrNB\")\r\n",
							"        # p = mul(config, a, b)\r\n",
							"        data_chunk_l = {'num1':[2, 3, 4, 5], 'num2':[20, 21, 19, 18]}\r\n",
							"        data_chunk = pd.DataFrame(data_chunk_l)\r\n",
							"        data_chunk_def_l = {'outp':[200, 3000, 4000, 50000]}\r\n",
							"        data_chunk_def = pd.DataFrame(data_chunk_def_l)\r\n",
							"        print(data_chunk)\r\n",
							"        fun = \"mul_Transpy\"\r\n",
							"        src_cols = [\"num1\",\"num2\"]\r\n",
							"        dst_cols = \"outp\"\r\n",
							"        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(current_module, str(fun)), axis = 1)\r\n",
							"        return data_chunk\r\n",
							"    else:\r\n",
							"        data_s = {'num1':[2, 3, 4, 5], 'num2':[20, 21, 19, 18]}\r\n",
							"        return data_s\r\n",
							"\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/source_downloader')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/edw/extraction_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5b6d27f8-2d6b-4f6a-abdd-e780dcd0640a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import requests\r\n",
							"import tarfile\r\n",
							"from io import BytesIO\r\n",
							"import os\r\n",
							"\r\n",
							"class DataDownloader:\r\n",
							"        \r\n",
							"    def download_data():\r\n",
							"        response = requests.get(\"https://static.crunchbase.com/data_crunchbase/bulk_export_sample.tar.gz\")\r\n",
							"        if response.status_code != 200:\r\n",
							"            print(\"Crunchbase Daily Data Export download failed \")\r\n",
							"            #raise Exception(f\"Crunchbase Daily Data Export download failed - {response.status_code}, {response.content} \")\r\n",
							"        \r\n",
							"        #self._logger.info(f\"Crunchbase Daily Data Export download completed.\")\r\n",
							"        data = BytesIO(response.content)\r\n",
							"        with tarfile.open(fileobj=data) as tar:\r\n",
							"            members = [tar.getmember('organizations.csv')] \r\n",
							"            #tar.extractall(dir_path, members=members)\r\n",
							"        data.close()\r\n",
							"        #self._logger.info(f\"Crunchbase Daily Data Export extraction completed.\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/src_downloader')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/crunchbase/extraction_operator"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "58e86349-550f-40d4-a2db-1481690b5ace"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import requests\r\n",
							"import tarfile\r\n",
							"from io import BytesIO\r\n",
							"import os\r\n",
							"\r\n",
							"class DataDownloader:\r\n",
							"    \r\n",
							"    def __init__(self, config, logger):\r\n",
							"        self._config = config\r\n",
							"        self._logger = logger\r\n",
							"        \r\n",
							"    def download_data(self, dir_path):\r\n",
							"        response = requests.get(self._config['crunchbase_uri'])\r\n",
							"        if response.status_code != 200:\r\n",
							"            raise Exception(f\"Crunchbase Daily Data Export download failed - {response.status_code}, {response.content} \")\r\n",
							"        \r\n",
							"        self._logger.info(f\"Crunchbase Daily Data Export download completed.\")\r\n",
							"        data = BytesIO(response.content)\r\n",
							"        with tarfile.open(fileobj=data) as tar:\r\n",
							"            members = [tar.getmember(member_name + '.csv') for member_name in self._config['entity_files']] \r\n",
							"            tar.extractall(dir_path, members=members)\r\n",
							"        data.close()\r\n",
							"        self._logger.info(f\"Crunchbase Daily Data Export extraction completed.\")\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/transformation_controller')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ce_pipelines/common"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "bmsparkpool2023",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "96f301e1-834f-49f3-8fef-df2c2f5fd6b1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a940a49d-4f38-49d5-8ee3-af225083b0bc/resourceGroups/EcosystemHub/providers/Microsoft.Synapse/workspaces/bmworkspace2023/bigDataPools/bmsparkpool2023",
						"name": "bmsparkpool2023",
						"type": "Spark",
						"endpoint": "https://bmworkspace2023.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmsparkpool2023",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ce_pipelines/common/az_table_connector"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-storage-file-datalake"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run ce_pipelines/common/datalake_connector"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# This call is not required since the config json content is passed from trans_run notebook to the TransformationController class\r\n",
							"# %run ce_pipelines/common/config"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# This call is not required since the config json content is passed from trans_run notebook to the TransformationController class\r\n",
							"# print(config[\"storage.account_name\"])\r\n",
							"# #print(e_config)\r\n",
							"# cm_config = config\r\n",
							"# print(cm_config)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# This call is not required since the config json content is passed from trans_run notebook to the TransformationController class\r\n",
							"# %run ce_pipelines/eoy/config/eoy_config"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# This call is not required since the config json content is passed from trans_run notebook to the TransformationController class\r\n",
							"# print(config[\"data.chunksize\"])\r\n",
							"# config.update(cm_config)\r\n",
							"# print(config)\r\n",
							"# print(config[\"storage.account_name\"])\r\n",
							"# print(config[\"storage.lookup_dir\"])"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storageaccountname)\r\n",
							"# storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storagecontainername)\r\n",
							"# storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"# print(storageaccountkey)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import tempfile\r\n",
							"import pandas as pd\r\n",
							"import re\r\n",
							"import csv\r\n",
							"import traceback\r\n",
							"from io import BytesIO \r\n",
							"\r\n",
							"class TransformationController:\r\n",
							"\r\n",
							"    def __init__(self, config, logger):\r\n",
							"        logger.info(f\"TransformationController - init - Started.\")\r\n",
							"        self.config = config\r\n",
							"        self._logger = logger\r\n",
							"        self._org_key_lookup = pd.DataFrame()\r\n",
							"        self._emp_key_lookup = pd.DataFrame()\r\n",
							"\r\n",
							"        self.storageaccountname = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        logger.info(f\"TransformationController acc name - {self.storageaccountname} \")\r\n",
							"        self.storagecontainername = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.container_name\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        logger.info(f\"TransformationController cntr nm - {self.storagecontainername} \")\r\n",
							"        self.storageaccountkey = mssparkutils.credentials.getSecret(config[\"keyvault.name\"],config[\"storage.account_key\"],config[\"linkedservice.keyvaultls\"])\r\n",
							"        logger.info(f\"TransformationController acc key - {self.storageaccountkey} \")\r\n",
							"        \r\n",
							"\r\n",
							"    def transform(self, file_path, entity):\r\n",
							"        logger.info(f\"TransformationController transform started - {file_path} \")\r\n",
							"        #file_name = 'eoy.csv'\r\n",
							"\r\n",
							"        #file_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"        #correct-file_path = 'abfss://'+storagecontainername+'@'+storageaccountname+'.dfs.core.windows.net/'+config[\"storage.staging_dir_name\"]+'/'+(file_name)\r\n",
							"\r\n",
							"\r\n",
							"        data_chunks  = pd.read_csv(file_path, chunksize=config[\"data.chunksize\"], dtype=str)\r\n",
							"\r\n",
							"        #file_path = 'abfss://test@bmdatalakestorage2023.dfs.core.windows.net/data/staging/eoy/eoy.csv'\r\n",
							"       \r\n",
							"        _transformation_lookup = config[\"transformations\"]\r\n",
							"        transformations = _transformation_lookup.get(entity,{}).get('mappings',[])\r\n",
							"        \r\n",
							"        final_chunk = []\r\n",
							"\r\n",
							"        for data_chunk in data_chunks:\r\n",
							"            \r\n",
							"            data_chunk.dropna(how='all', inplace=True)\r\n",
							"            data_chunk.fillna('', inplace=True)\r\n",
							"            for trfm in transformations:\r\n",
							"                try:\r\n",
							"                    \r\n",
							"                    fun = trfm[\"function\"]\r\n",
							"                    src_cols = trfm[\"src_column_names\"]\r\n",
							"                    dst_cols = trfm[\"dst_column_names\"]\r\n",
							"                    \r\n",
							"                    if \"threshold\" in trfm:\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun), args=(trfm[\"threshold\"],))\r\n",
							"                    elif isinstance(src_cols, list):\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun), axis=1)\r\n",
							"                    else:\r\n",
							"                        \r\n",
							"                        data_chunk[dst_cols] = data_chunk[src_cols].apply(getattr(self, fun))\r\n",
							"                except Exception as e:\r\n",
							"                    self._logger.error(str(e))\r\n",
							"                    traceback.print_exc()\r\n",
							"\r\n",
							"\r\n",
							"            final_chunk.append(data_chunk)\r\n",
							"        if final_chunk:\r\n",
							"            _data = pd.concat(final_chunk, ignore_index=True)\r\n",
							"\r\n",
							"            invalid_columns = _transformation_lookup.get('invalid_columns', [])\r\n",
							"            if invalid_columns:\r\n",
							"                _data.drop(invalid_columns, axis=1, inplace=True)\r\n",
							"\r\n",
							"            #local_file_path = os.path.join(tempfile.gettempdir(), f\"{entity}_trfm.csv\")\r\n",
							"            #dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+entity+'eoy_trfm.csv'\r\n",
							"            #dest_path = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"            temp_path = 'abfss://'+self.storagecontainername+'@'+self.storageaccountname+'.dfs.core.windows.net/'+config[\"storage.temp_dir\"]+'/'+config[\"source_flag\"]+'/'+entity+'_trfm.csv'\r\n",
							"\r\n",
							"            \r\n",
							"            data = pd.DataFrame(_data)\r\n",
							"            #data.to_csv(dest_path,index=False,storage_options = {'account_key':'Wl+cozJ6xDQtnstjWpmHpz/2UGxgaptMpDl1/OMPiLUQS+2fuNdeqMmnvzgCTXTwL7c3oNYjrRvl+AStI26Zaw=='})\r\n",
							"            data.to_csv(temp_path,index=False,storage_options = {'account_key':self.storageaccountkey})\r\n",
							"\r\n",
							"            return temp_path  \r\n",
							"        \r\n",
							"        #else:\r\n",
							"            #return None\r\n",
							"\r\n",
							"    geo_type = {2:'city',1:'region',0:'country'}\r\n",
							"    geo_prefix = {2:'CI',1:'RE',0:'CO'}\r\n",
							"\r\n",
							"    def transform_geo(self, geo_data):\r\n",
							"        # input - geo_data - list Order - country_code,region,city \r\n",
							"    \r\n",
							"        inc_id = []\r\n",
							"        inc_geo_nodes = []\r\n",
							"        for idx, ge in enumerate(geo_data):\r\n",
							"            if ge:\r\n",
							"                \r\n",
							"                raw_ge = re.sub(r'\\W','',ge)\r\n",
							"                inc_id.append(self.geo_prefix[idx])\r\n",
							"                inc_id.append(raw_ge[0:30].upper())\r\n",
							"                \r\n",
							"                inc_geo_nodes.append(f\"{''.join(inc_id[0:-2])};{''.join(inc_id)};{self.geo_type[idx]};{ge.strip()};{raw_ge.lower()}\")\r\n",
							"                \r\n",
							"        return pd.Series([''.join(inc_id), inc_geo_nodes[-1], '|'.join(inc_geo_nodes)])\r\n",
							"    \r\n",
							"    def ey_entity_check(self, val):\r\n",
							"        return 1 if val in self.config['crunchbase.ey_uuid'] else 0\r\n",
							"    \r\n",
							"    def normalize(self, val):\r\n",
							"        return re.sub(r\"[^a-z 0-9]\",\"\",val.lower()).strip() if val else ''\r\n",
							"    \r\n",
							"    def remove_unknowns(self, val):\r\n",
							"        return None if str(val).lower().strip() == 'unknown' else val\r\n",
							"\r\n",
							"    def remove_spl(self, val):\r\n",
							"        if (str(val) != 'nan') and ( str(val) != 'None'):\r\n",
							"            s = re.sub(r\"[^a-z 0-9]\",\"\",val.lower()).strip()\r\n",
							"        else:\r\n",
							"            s = None\r\n",
							"        return s\r\n",
							"\r\n",
							"    def link_preprocessing(self, val, threshold):\r\n",
							"        if (str(val) != 'nan') and ( str(val) != 'None'):\r\n",
							"            if val[-1] == '/':\r\n",
							"                ret = val[:-1]\r\n",
							"            spltval = val.split('/')\r\n",
							"            i = 0\r\n",
							"            retV = \"\"\r\n",
							"            if len(spltval) >= (threshold):\r\n",
							"                while i<threshold:\r\n",
							"                    retV = retV + spltval[i] + \"/\"\r\n",
							"                    i = i+1\r\n",
							"                retV = retV[:-1]\r\n",
							"        else:\r\n",
							"            retV = None\r\n",
							"        return pd.Series([retV])\r\n",
							"    \r\n",
							"    def get_value(self, file_name: str, lookup_field: str, searched_field: str, lookup_value, default):\r\n",
							"        source = self.config['source_flag']\r\n",
							"        lookup_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"lookup_files\",f\"{source}_{file_name}\")\r\n",
							"        try:\r\n",
							"            lookup_df = pd.read_csv(lookup_file_path, dtype=str)\r\n",
							"        except pd.errors.ParserError as e:\r\n",
							"            self._logger.warning(f\"{str(e)}, Attempting advanced parsing\")\r\n",
							"            lookup_df = pd.read_csv(lookup_file_path, engine='python', quoting=csv.QUOTE_NONE, dtype=str)\r\n",
							"        candidates = lookup_df[(lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        if not candidates.empty:\r\n",
							"            for elem in candidates:\r\n",
							"                value = elem\r\n",
							"                break\r\n",
							"            return value\r\n",
							"        else:\r\n",
							"            return default\r\n",
							"\r\n",
							"    def get_values(self, file_name: str, lookup_field: str, searched_field: list, lookup_value, source_match=False, default=\"\"):\r\n",
							"        source = self.config['source_flag']\r\n",
							"        \r\n",
							"        #file_name = 'org_key_lookup.csv'\r\n",
							"       \r\n",
							"        #file_path2 = 'abfss://'+config[\"storage.container_name\"]+'@'+config[\"storage.account_name\"]+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"        file_path2 = 'abfss://'+self.storagecontainername+'@'+self.storageaccountname+'.dfs.core.windows.net/'+config[\"storage.lookup_dir\"]+'/'+config['source_flag']+'_'+(file_name)\r\n",
							"\r\n",
							"        #lookup_df = cache[file_name] if cache else None\r\n",
							"        if 1==1:\r\n",
							"\r\n",
							"\r\n",
							"            #self._logger.info(f\"{file_name} - Reading from file\")\r\n",
							"\r\n",
							"            #lookup_dir = self.config['storage.lookup_dir']\r\n",
							"            #df_data = DataLakeAdapter(self.config, self._logger).download_file_blob(file_name=f\"{source}_{file_name}\", file_path=lookup_dir)\r\n",
							"            #lookup_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"lookup_files\",f\"{source}_{file_name}\")\r\n",
							"            \r\n",
							"            try:\r\n",
							"                lookup_df = pd.read_csv(file_path2, dtype=str)\r\n",
							"            except pd.errors.ParserError as e:\r\n",
							"                self._logger.warning(f\"{str(e)}, Attempting advanced parsing\")\r\n",
							"                lookup_df = pd.read_csv(file_path2, quoting=csv.QUOTE_NONE, dtype=str)\r\n",
							"\r\n",
							"\r\n",
							"        if source_match:\r\n",
							"            candidates = lookup_df[(lookup_df['source'] == source) & (lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        else:\r\n",
							"            candidates = lookup_df[(lookup_df[lookup_field] == lookup_value)][searched_field]\r\n",
							"        \r\n",
							"        value = default\r\n",
							"        if not candidates.empty:\r\n",
							"            if isinstance(searched_field, list):\r\n",
							"                for idx, elem in candidates.iterrows():\r\n",
							"                    res = []\r\n",
							"                    for sf in searched_field:\r\n",
							"                        res.append(elem[sf])\r\n",
							"                    value = ';'.join(res)\r\n",
							"                    break\r\n",
							"            else:\r\n",
							"                for elem in candidates:\r\n",
							"                    value = elem\r\n",
							"\r\n",
							"                    break\r\n",
							"                    \r\n",
							"        return value\r\n",
							"\r\n",
							"\r\n",
							"    # Generate org_key\r\n",
							"    def get_org_key(self, source_id):\r\n",
							"        \r\n",
							"        source = self.config['source_flag']\r\n",
							"    \r\n",
							"\r\n",
							"        return self.get_values(file_name=\"org_key_lookup.csv\", \r\n",
							"                            lookup_field=\"source_id\", \r\n",
							"                            searched_field=\"key\", \r\n",
							"                            lookup_value=source_id, \r\n",
							"                            source_match=True, default=source + str(source_id))\r\n",
							"    \r\n",
							"    # Generate emp_key\r\n",
							"    def get_emp_key(self, source_id):\r\n",
							"        \r\n",
							"        source = self.config['source_flag']\r\n",
							"        \r\n",
							"        return self.get_values(file_name=\"emp_key_lookup.csv\", \r\n",
							"                            lookup_field=\"source_id\", \r\n",
							"                            searched_field=\"key\", \r\n",
							"                            lookup_value=source_id, \r\n",
							"                            source_match=True, default=source + str(source_id))\r\n",
							"        \r\n",
							"\r\n",
							"    def get_table_values(self,partition_key,row_key,columns):\r\n",
							"        try:\r\n",
							"            result_query = TableConnector(self.config).search(partition_key=partition_key,row_key=row_key,columns=columns)\r\n",
							"\r\n",
							"            lst = []\r\n",
							"            for col in columns:\r\n",
							"                lst.append(result_query.get(col,\"\"))\r\n",
							"\r\n",
							"            result = \";\".join(lst)\r\n",
							"\r\n",
							"\r\n",
							"        except Exception as err:\r\n",
							"            raise Exception('Error in extracting entities from azure table:{}'.format(err))\r\n",
							"\r\n",
							"        return result\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import logging\r\n",
							"# logger config\r\n",
							"# log levels: debug, info, warning, error, critical\r\n",
							"logFormatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s',datefmt='%Y-%m-%d %H:%M:%S')\r\n",
							"logger = logging.getLogger('EOY')\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"# stream logger\r\n",
							"sh = logging.StreamHandler() # stream handler\r\n",
							"sh.setFormatter(logFormatter)\r\n",
							"sh.setLevel(logging.INFO)\r\n",
							"logger.addHandler(sh)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#obj = TransformationController(config,logger)"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bmsparkpool2023')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		}
	]
}