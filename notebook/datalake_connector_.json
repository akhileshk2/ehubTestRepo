{
	"name": "datalake_connector_",
	"properties": {
		"folder": {
			"name": "AK W"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "bmapachespark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9edc5e0b-147e-42d1-8463-e241d88313d1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8e862a61-456c-49b5-9e16-45c00c15686b/resourceGroups/synapsepoc/providers/Microsoft.Synapse/workspaces/bmworkspace2022/bigDataPools/bmapachespark",
				"name": "bmapachespark",
				"type": "Spark",
				"endpoint": "https://bmworkspace2022.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/bmapachespark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.filedatalake import DataLakeServiceClient\r\n",
					"from azure.storage.blob import generate_account_sas, ResourceTypes, AccountSasPermissions\r\n",
					"\r\n",
					"import os\r\n",
					"import tempfile\r\n",
					"import pandas as pd\r\n",
					"import io\r\n",
					"from datetime import datetime, timedelta\r\n",
					"\r\n",
					"class DataLakeAdapter:\r\n",
					"    \r\n",
					"    def __init__(self, config, logger, env=None, entity=None):\r\n",
					"        self._config = config\r\n",
					"        self._logger = logger\r\n",
					"        self._storage_url = \"{}://{}.dfs.core.windows.net\".format(\"https\", self._config[\"storage.account_name\"])\r\n",
					"        self._storage_credential = self._config[\"storage.account_key\"]\r\n",
					"        self._storage_container = self._config[\"storage.container_name\"]\r\n",
					"        \r\n",
					"        if env:\r\n",
					"            self._init_directory_structure(env, entity)\r\n",
					"\r\n",
					"    def _init_directory_structure(self, env, entity=None):\r\n",
					"        file_system_client = DataLakeServiceClient(\r\n",
					"            account_url=self._storage_url, \r\n",
					"            credential=self._storage_credential\r\n",
					"        ).get_file_system_client(file_system=self._storage_container)\r\n",
					"\r\n",
					"        dir_path=self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else '')\r\n",
					"\r\n",
					"        try:\r\n",
					"            file_system_client.delete_directory(dir_path)\r\n",
					"        except Exception as e:\r\n",
					"            if 'PathNotFound' in str(e):\r\n",
					"                self._logger.info(f'Creating new dir {dir_path}')\r\n",
					"            else:\r\n",
					"                self._logger.error(f'Delete dir {dir_path} failed with exception, {e}')\r\n",
					"                raise e\r\n",
					"\r\n",
					"        file_system_client.create_directory(dir_path)\r\n",
					"        file_system_client.close()\r\n",
					"        self._logger.info(f'New dir created {dir_path}')\r\n",
					"\r\n",
					"    def upload_file(self, file_name, file_data, env, entity=None):\r\n",
					"        file_system_client = DataLakeServiceClient(\r\n",
					"            account_url=self._storage_url, \r\n",
					"            credential=self._storage_credential\r\n",
					"        ).get_file_system_client(file_system=self._storage_container)\r\n",
					"\r\n",
					"        dir_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"] + (f\"/{entity}\" if entity else ''))\r\n",
					"        file_client = dir_client.get_file_client(file_name)\r\n",
					"        file_data.seek(0)\r\n",
					"        file_client.upload_data(file_data.getvalue(), overwrite=True)\r\n",
					"        file_client.close()\r\n",
					"        dir_client.close()\r\n",
					"        file_system_client.close()\r\n",
					"        self._logger.info(f'File upload completed, {file_name}') \r\n",
					"\r\n",
					"    def download_file_blob(self, file_name, file_path):\r\n",
					"        file_system_client = DataLakeServiceClient(\r\n",
					"            account_url=self._storage_url, \r\n",
					"            credential=self._storage_credential\r\n",
					"        ).get_file_system_client(file_system=self._storage_container)\r\n",
					"\r\n",
					"        dir_client = file_system_client.get_directory_client(file_path)\r\n",
					"        file_client = dir_client.get_file_client(file_name)\r\n",
					"        file_hndlr = file_client.download_file()\r\n",
					"        file_data = file_hndlr.readall()\r\n",
					"        self._logger.info(f'File download completed, {file_name}') \r\n",
					"        return file_data\r\n",
					"\r\n",
					"    def split_upload_file_from_local(self, source_file_path, entity, env='landing'):\r\n",
					"        try:\r\n",
					"            chunksize = self._config[\"storage.chunksize\"]\r\n",
					"            fileuploadname = 'part-'\r\n",
					"            data_chunks  = pd.read_csv(source_file_path, chunksize=chunksize, dtype=str)\r\n",
					"            _file_cnt = 0\r\n",
					"            for chunks in data_chunks:\r\n",
					"                \r\n",
					"                data = io.StringIO()\r\n",
					"                chunks.to_csv(data, index=False)\r\n",
					"                uploadFileName = fileuploadname + str(_file_cnt).rjust(5,'0')\r\n",
					"                self.upload_file(f'{uploadFileName}.csv', data, env, entity=entity)\r\n",
					"                data.close()\r\n",
					"                _file_cnt+=1\r\n",
					"\r\n",
					"            self._logger.info(f'File Split upload completed for entity - {entity}; No. of parts {_file_cnt}') \r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            self._logger.error(e)   \r\n",
					"\r\n",
					"    def download_file(self, file_name, dest_dir_path=tempfile.gettempdir(), env=None):\r\n",
					"        try:\r\n",
					"            file_system_client = DataLakeServiceClient(\r\n",
					"                account_url=self._storage_url, \r\n",
					"                credential=self._storage_credential\r\n",
					"            ).get_file_system_client(file_system=self._storage_container)\r\n",
					"\r\n",
					"            directory_client = file_system_client.get_directory_client(self._config[f\"storage.{env}_dir_name\"])\r\n",
					"            file_client = directory_client.get_file_client(file_name)\r\n",
					"            download = file_client.download_file()\r\n",
					"            downloaded_bytes = download.readall()\r\n",
					"            file_path = os.path.join(dest_dir_path, file_name)\r\n",
					"            local_file = open(file_path,'wb')\r\n",
					"            local_file.write(downloaded_bytes)\r\n",
					"            local_file.close()\r\n",
					"            self._logger.info(f'File download completed, {file_name}') \r\n",
					"            return file_path\r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            self._logger.error(e)\r\n",
					"            return None\r\n",
					"\r\n",
					"    def get_files(self, env, entity):\r\n",
					"        \r\n",
					"        try:\r\n",
					"            file_system_client = DataLakeServiceClient(\r\n",
					"                account_url=self._storage_url, \r\n",
					"                credential=self._storage_credential\r\n",
					"            ).get_file_system_client(file_system=self._storage_container)\r\n",
					"\r\n",
					"            env_path = self._config[f\"storage.{env}_dir_name\"]\r\n",
					"            paths = file_system_client.get_paths(path=f\"{env_path}/{entity}\")\r\n",
					"\r\n",
					"            return [p.name for p in paths]\r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            self._logger.error(e)\r\n",
					"                \r\n",
					"    def generate_sas_url(self, file_path):\r\n",
					"\r\n",
					"        sas_token = generate_account_sas(\r\n",
					"            account_name=self._config[\"storage.account_name\"],\r\n",
					"            account_key=self._config[\"storage.account_key\"],\r\n",
					"            resource_types=ResourceTypes(object=True,),\r\n",
					"            permission=AccountSasPermissions(read=True),\r\n",
					"            expiry=datetime.utcnow() + timedelta(hours=1)\r\n",
					"        )\r\n",
					"\r\n",
					"        return f\"https://{self._config['storage.account_name']}.blob.core.windows.net/{self._storage_container}/{file_path}?{sas_token}\"\r\n",
					""
				],
				"execution_count": 21
			}
		]
	}
}